{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2282ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from time import time\n",
    "import joblib\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer,\n",
    ")  # Required for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272b0ce",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d332b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"cv_folds\": 5,\n",
    "    \"scale_data\": True,\n",
    "    \"handle_imbalance\": True,\n",
    "    \"imputation_strategy\": \"iterative\",  # 'iterative', 'knn', or 'simple'\n",
    "    \"n_jobs\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49c9ef",
   "metadata": {},
   "source": [
    "#### ADVANCED IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e8e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedImputer:\n",
    "    \"\"\"\n",
    "    Advanced imputation strategies for missing data.\n",
    "\n",
    "    Strategies:\n",
    "    1. 'iterative' - MICE (Multivariate Imputation by Chained Equations)\n",
    "       - Models each feature with missing values as a function of other features\n",
    "       - Iteratively predicts missing values\n",
    "       - Best for complex relationships\n",
    "\n",
    "    2. 'knn' - K-Nearest Neighbors Imputation\n",
    "       - Fills missing values using k-nearest neighbors\n",
    "       - Good for preserving local structure\n",
    "\n",
    "    3. 'simple' - Statistical imputation\n",
    "       - Uses median for numerical features\n",
    "       - Fallback option\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, strategy=\"iterative\", n_neighbors=5, max_iter=10, random_state=42\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.imputer = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the imputer on training data\"\"\"\n",
    "        print(f\"\\nğŸ”§ Fitting {self.strategy.upper()} imputer...\")\n",
    "\n",
    "        if self.strategy == \"iterative\":\n",
    "            # MICE - Models each feature as a function of others\n",
    "            # Uses BayesianRidge as the estimator (fast and robust)\n",
    "            self.imputer = IterativeImputer(\n",
    "                estimator=BayesianRidge(),\n",
    "                max_iter=self.max_iter,\n",
    "                random_state=self.random_state,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "        elif self.strategy == \"knn\":\n",
    "            # KNN - Uses nearest neighbors to fill missing values\n",
    "            self.imputer = KNNImputer(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                weights=\"distance\",  # Closer neighbors have more influence\n",
    "            )\n",
    "\n",
    "        elif self.strategy == \"simple\":\n",
    "            # Simple statistical imputation (fallback)\n",
    "            self.imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown imputation strategy: {self.strategy}\")\n",
    "\n",
    "        self.imputer.fit(X)\n",
    "        print(f\"âœ“ Imputer fitted successfully\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted imputer\"\"\"\n",
    "        if self.imputer is None:\n",
    "            raise ValueError(\"Imputer must be fitted before transform\")\n",
    "\n",
    "        # Get column names if DataFrame\n",
    "        columns = X.columns if isinstance(X, pd.DataFrame) else None\n",
    "        index = X.index if isinstance(X, pd.DataFrame) else None\n",
    "\n",
    "        # Transform\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "\n",
    "        # Convert back to DataFrame if input was DataFrame\n",
    "        if columns is not None:\n",
    "            X_imputed = pd.DataFrame(X_imputed, columns=columns, index=index)\n",
    "\n",
    "        return X_imputed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_and_invalid_values(\n",
    "    X, strategy=\"iterative\", fit_imputer=True, imputer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Handle missing values, infinities, and invalid data with advanced imputation.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix (DataFrame)\n",
    "        strategy: Imputation strategy ('iterative', 'knn', 'simple')\n",
    "        fit_imputer: Whether to fit a new imputer (True for train, False for test)\n",
    "        imputer: Pre-fitted imputer (for test data)\n",
    "\n",
    "    Returns:\n",
    "        X_clean: Cleaned feature matrix\n",
    "        imputer: Fitted imputer (if fit_imputer=True)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HANDLING MISSING AND INVALID VALUES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    original_shape = X.shape\n",
    "\n",
    "    # Step 2: Check for missing values\n",
    "    print(\"\\n2. Checking for missing values...\")\n",
    "    missing_count = X.isnull().sum().sum()\n",
    "    missing_cols = X.columns[X.isnull().any()].tolist()\n",
    "\n",
    "    if missing_count > 0:\n",
    "        print(\n",
    "            f\"   âš ï¸  Found {missing_count} missing values in {len(missing_cols)} columns\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Missing percentage: {(missing_count / (X.shape[0] * X.shape[1]) * 100):.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Show top columns with missing values\n",
    "        missing_per_col = X.isnull().sum().sort_values(ascending=False)\n",
    "        top_missing = missing_per_col[missing_per_col > 0].head(10)\n",
    "        print(f\"\\n   Top columns with missing values:\")\n",
    "        for col, count in top_missing.items():\n",
    "            pct = (count / len(X)) * 100\n",
    "            print(f\"     â€¢ {col}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "        # Step 3: Apply advanced imputation\n",
    "        print(f\"\\n3. Applying {strategy.upper()} imputation...\")\n",
    "\n",
    "        if fit_imputer:\n",
    "            # Fit new imputer on training data\n",
    "            imputer = AdvancedImputer(\n",
    "                strategy=strategy, random_state=CONFIG[\"random_state\"]\n",
    "            )\n",
    "            X_clean = imputer.fit_transform(X)\n",
    "        else:\n",
    "            # Use pre-fitted imputer on test data\n",
    "            if imputer is None:\n",
    "                raise ValueError(\"Imputer must be provided for test data\")\n",
    "            X_clean = imputer.transform(X)\n",
    "\n",
    "        # Verify no missing values remain\n",
    "        remaining_missing = (\n",
    "            pd.DataFrame(X_clean, columns=X.columns).isnull().sum().sum()\n",
    "        )\n",
    "        if remaining_missing > 0:\n",
    "            print(\n",
    "                f\"   âš ï¸  WARNING: {remaining_missing} missing values remain after imputation\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   âœ“ All missing values imputed successfully\")\n",
    "\n",
    "    else:\n",
    "        print(f\"   âœ“ No missing values found\")\n",
    "        X_clean = X\n",
    "\n",
    "    # Step 4: Final validation\n",
    "    print(\"\\n4. Final validation...\")\n",
    "    X_clean_df = pd.DataFrame(X_clean, columns=X.columns, index=X.index)\n",
    "\n",
    "    # Check for any remaining issues\n",
    "    has_nan = X_clean_df.isnull().any().any()\n",
    "\n",
    "    if has_nan:\n",
    "        print(f\"   âš ï¸  WARNING: Data still contains invalid values!\")\n",
    "        print(f\"      NaN: {has_nan}\")\n",
    "        # Emergency fallback - fill with 0\n",
    "        X_clean_df = X_clean_df.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        print(f\"   âœ“ Applied emergency fallback (fill with 0)\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Data is clean and ready for training\")\n",
    "\n",
    "    print(f\"\\n5. Summary:\")\n",
    "    print(f\"   Original shape: {original_shape}\")\n",
    "    print(f\"   Final shape: {X_clean_df.shape}\")\n",
    "    print(f\"   Features preserved: {X_clean_df.shape[1]}\")\n",
    "\n",
    "    if fit_imputer:\n",
    "        return X_clean_df, imputer\n",
    "    else:\n",
    "        return X_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eccb31",
   "metadata": {},
   "source": [
    "#### 1. LOAD AND EXPLORE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc4af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(filepath):\n",
    "    \"\"\"Load data and show basic statistics\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"âœ“ Dataset loaded: {df.shape}\")\n",
    "\n",
    "    # Check for issues\n",
    "    missing = df.isnull().sum().sum()\n",
    "    duplicates = df.duplicated().sum()\n",
    "\n",
    "    print(f\"\\nData Quality Check:\")\n",
    "    print(\n",
    "        f\"  Missing values: {missing} ({(missing/(df.shape[0]*df.shape[1])*100):.2f}%)\"\n",
    "    )\n",
    "    print(f\"  Duplicate rows: {duplicates}\")\n",
    "\n",
    "    if missing > 0:\n",
    "        print(f\"  â„¹ï¸  Will apply advanced imputation in next step\")\n",
    "\n",
    "    # Class distribution\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(df[\"result\"].value_counts())\n",
    "    print(f\"Phishing ratio: {df['result'].mean():.2%}\")\n",
    "\n",
    "    # Calculate imbalance ratio\n",
    "    class_counts = df[\"result\"].value_counts()\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "    if imbalance_ratio > 1.5:\n",
    "        print(\"âš ï¸  Dataset is imbalanced - will apply SMOTE\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f64aa",
   "metadata": {},
   "source": [
    "#### 2. PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2db0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, config):\n",
    "    \"\"\"Prepare features and labels, handle scaling and imbalance\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: PREPARING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = df.drop([\"rec_id\", \"result\"], axis=1)\n",
    "    y = df[\"result\"]\n",
    "\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"  Features: {X.shape[1]}\")\n",
    "    print(f\"  Samples: {len(X)}\")\n",
    "    print(f\"  Classes: {y.nunique()}\")\n",
    "\n",
    "    # Split data BEFORE imputation (important!)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=config[\"test_size\"],\n",
    "        random_state=config[\"random_state\"],\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nData Split:\")\n",
    "    print(f\"  Train set: {X_train.shape}\")\n",
    "    print(f\"  Test set: {X_test.shape}\")\n",
    "\n",
    "    # Apply advanced imputation\n",
    "    X_train, imputer = handle_missing_and_invalid_values(\n",
    "        X_train, strategy=config[\"imputation_strategy\"], fit_imputer=True\n",
    "    )\n",
    "\n",
    "    X_test = handle_missing_and_invalid_values(\n",
    "        X_test,\n",
    "        strategy=config[\"imputation_strategy\"],\n",
    "        fit_imputer=False,\n",
    "        imputer=imputer,\n",
    "    )\n",
    "\n",
    "    # Scale data if configured\n",
    "    scaler = None\n",
    "    if config[\"scale_data\"]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FEATURE SCALING\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nâœ“ Scaling features with RobustScaler...\")\n",
    "        print(f\"  (RobustScaler is robust to outliers using median and IQR)\")\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        X_train = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "        X_test = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "        print(f\"  âœ“ Features scaled successfully\")\n",
    "\n",
    "    # Handle class imbalance if configured\n",
    "    if config[\"handle_imbalance\"]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"HANDLING CLASS IMBALANCE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        class_counts = y_train.value_counts()\n",
    "        imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "\n",
    "        print(f\"\\nCurrent class distribution:\")\n",
    "        print(f\"  Class 0 (Legit): {class_counts.get(0, 0):,}\")\n",
    "        print(f\"  Class 1 (Phishing): {class_counts.get(1, 0):,}\")\n",
    "        print(f\"  Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "        if imbalance_ratio > 1.5:\n",
    "            print(f\"\\nâœ“ Applying SMOTE (Synthetic Minority Over-sampling)...\")\n",
    "            print(f\"  (Generates synthetic samples for minority class)\")\n",
    "\n",
    "            original_size = len(X_train)\n",
    "\n",
    "            smote = SMOTE(random_state=config[\"random_state\"], k_neighbors=5)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "            new_counts = y_train.value_counts()\n",
    "            print(f\"\\n  Results:\")\n",
    "            print(f\"    Before: {original_size:,} samples\")\n",
    "            print(f\"    After: {len(X_train):,} samples\")\n",
    "            print(f\"    Class 0: {new_counts.get(0, 0):,}\")\n",
    "            print(f\"    Class 1: {new_counts.get(1, 0):,}\")\n",
    "            print(f\"    New ratio: {new_counts.max() / new_counts.min():.2f}:1\")\n",
    "        else:\n",
    "            print(f\"\\nâœ“ Classes are balanced enough, skipping SMOTE\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler, imputer, X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777615a9",
   "metadata": {},
   "source": [
    "#### 3. DEFINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fc6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(config):\n",
    "    \"\"\"Define all models to train\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: DEFINING MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    models = {\n",
    "        # Tree-based ensembles (best for this task)\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=config[\"random_state\"],\n",
    "            n_jobs=config[\"n_jobs\"],\n",
    "        ),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            random_state=config[\"random_state\"],\n",
    "            n_jobs=config[\"n_jobs\"],\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=config[\"random_state\"],\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=config[\"random_state\"],\n",
    "            n_jobs=config[\"n_jobs\"],\n",
    "            eval_metric=\"logloss\",\n",
    "        ),\n",
    "        \"LightGBM\": LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=config[\"random_state\"],\n",
    "            n_jobs=config[\"n_jobs\"],\n",
    "            verbose=-1,\n",
    "        ),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            depth=10,\n",
    "            learning_rate=0.1,\n",
    "            random_state=config[\"random_state\"],\n",
    "            verbose=False,\n",
    "        ),\n",
    "        # Linear models\n",
    "        # \"Logistic Regression\": LogisticRegression(\n",
    "        #     max_iter=1000, random_state=config[\"random_state\"], n_jobs=config[\"n_jobs\"]\n",
    "        # ),\n",
    "        # Other models\n",
    "        # \"SVC\": SVC(kernel=\"rbf\", probability=True, random_state=config[\"random_state\"]),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=config[\"n_jobs\"]),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(\n",
    "            max_depth=20, min_samples_split=5, random_state=config[\"random_state\"]\n",
    "        ),\n",
    "        \"AdaBoost\": AdaBoostClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, random_state=config[\"random_state\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(f\"âœ“ Configured {len(models)} models:\")\n",
    "    for name in models.keys():\n",
    "        print(f\"  â€¢ {name}\")\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05937611",
   "metadata": {},
   "source": [
    "#### 4. TRAIN AND EVALUATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44656c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a single model\"\"\"\n",
    "\n",
    "    # Train\n",
    "    start_time = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start_time\n",
    "\n",
    "    # Predict\n",
    "    start_time = time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    predict_time = time() - start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba),\n",
    "        \"train_time\": train_time,\n",
    "        \"predict_time\": predict_time,\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    metrics[\"tn\"] = cm[0, 0]\n",
    "    metrics[\"fp\"] = cm[0, 1]\n",
    "    metrics[\"fn\"] = cm[1, 0]\n",
    "    metrics[\"tp\"] = cm[1, 1]\n",
    "\n",
    "    # Error rates\n",
    "    metrics[\"fpr\"] = (\n",
    "        cm[0, 1] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0\n",
    "    )\n",
    "    metrics[\"fnr\"] = (\n",
    "        cm[1, 0] / (cm[1, 0] + cm[1, 1]) if (cm[1, 0] + cm[1, 1]) > 0 else 0\n",
    "    )\n",
    "\n",
    "    return metrics, model, y_pred, y_proba\n",
    "\n",
    "\n",
    "def train_all_models(models, X_train, X_test, y_train, y_test, config):\n",
    "    \"\"\"Train all models and collect results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: TRAINING MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'â”€' * 80}\")\n",
    "        print(f\"Training: {name}\")\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "\n",
    "        try:\n",
    "            metrics, trained_model, y_pred, y_proba = evaluate_model(\n",
    "                model, X_train, X_test, y_train, y_test, name\n",
    "            )\n",
    "\n",
    "            results[name] = {\n",
    "                \"model\": trained_model,\n",
    "                \"metrics\": metrics,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_proba\": y_proba,\n",
    "            }\n",
    "\n",
    "            # Print results\n",
    "            print(f\"âœ“ Training completed in {metrics['train_time']:.2f}s\")\n",
    "            print(f\"\\nPerformance Metrics:\")\n",
    "            print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "            print(f\"\\nConfusion Matrix:\")\n",
    "            print(f\"  TN: {metrics['tn']:,} | FP: {metrics['fp']:,}\")\n",
    "            print(f\"  FN: {metrics['fn']:,} | TP: {metrics['tp']:,}\")\n",
    "            print(f\"\\nError Rates:\")\n",
    "            print(f\"  FPR: {metrics['fpr']:.4f} (legit sites flagged as phishing)\")\n",
    "            print(f\"  FNR: {metrics['fnr']:.4f} (phishing sites missed)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Training failed: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c8b21",
   "metadata": {},
   "source": [
    "#### 5. COMPARE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10adefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results):\n",
    "    \"\"\"Compare all models and find the best one\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    comparison = []\n",
    "    for name, result in results.items():\n",
    "        metrics = result[\"metrics\"]\n",
    "        comparison.append(\n",
    "            {\n",
    "                \"Model\": name,\n",
    "                \"Accuracy\": metrics[\"accuracy\"],\n",
    "                \"Precision\": metrics[\"precision\"],\n",
    "                \"Recall\": metrics[\"recall\"],\n",
    "                \"F1-Score\": metrics[\"f1\"],\n",
    "                \"ROC-AUC\": metrics[\"roc_auc\"],\n",
    "                \"Train Time (s)\": metrics[\"train_time\"],\n",
    "                \"FPR\": metrics[\"fpr\"],\n",
    "                \"FNR\": metrics[\"fnr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_comparison = pd.DataFrame(comparison)\n",
    "    df_comparison = df_comparison.sort_values(\"ROC-AUC\", ascending=False)\n",
    "\n",
    "    print(\"\\nğŸ“Š Model Performance Summary (sorted by ROC-AUC):\")\n",
    "    print(\"â”€\" * 80)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", None)\n",
    "    pd.set_option(\"display.precision\", 4)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = df_comparison.iloc[0][\"Model\"]\n",
    "    best_metrics = results[best_model_name][\"metrics\"]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ† BEST MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nModel: {best_model_name}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(\n",
    "        f\"  âœ“ Accuracy:  {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  âœ“ Precision: {best_metrics['precision']:.4f} ({best_metrics['precision']*100:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  âœ“ Recall:    {best_metrics['recall']:.4f} ({best_metrics['recall']*100:.2f}%)\"\n",
    "    )\n",
    "    print(f\"  âœ“ F1-Score:  {best_metrics['f1']:.4f}\")\n",
    "    print(f\"  âœ“ ROC-AUC:   {best_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(\n",
    "        f\"  True Negatives:  {best_metrics['tn']:,} (legit sites correctly identified)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  False Positives: {best_metrics['fp']:,} (legit sites flagged as phishing)\"\n",
    "    )\n",
    "    print(f\"  False Negatives: {best_metrics['fn']:,} (phishing sites missed) âš ï¸\")\n",
    "    print(f\"  True Positives:  {best_metrics['tp']:,} (phishing sites caught)\")\n",
    "\n",
    "    print(f\"\\nError Analysis:\")\n",
    "    print(\n",
    "        f\"  FPR: {best_metrics['fpr']:.4f} ({best_metrics['fpr']*100:.2f}% of legit sites flagged)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  FNR: {best_metrics['fnr']:.4f} ({best_metrics['fnr']*100:.2f}% of phishing sites missed)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSpeed:\")\n",
    "    print(f\"  Training: {best_metrics['train_time']:.2f}s\")\n",
    "    print(f\"  Prediction: {best_metrics['predict_time']*1000:.2f}ms\")\n",
    "\n",
    "    return best_model_name, df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319b787",
   "metadata": {},
   "source": [
    "#### 6. SAVE ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(best_model_name, results, scaler, imputer, feature_names):\n",
    "    \"\"\"Save model and related artifacts\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 7: SAVING ARTIFACTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Save best model\n",
    "    best_model = results[best_model_name][\"model\"]\n",
    "    joblib.dump(\n",
    "        best_model,\n",
    "        \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/best_phishing_model_webcode.pkl\",\n",
    "    )\n",
    "    print(f\"âœ“ Saved best model: best_phishing_model_webcode.pkl\")\n",
    "\n",
    "    # Save scaler\n",
    "    if scaler is not None:\n",
    "        joblib.dump(\n",
    "            scaler,\n",
    "            \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/feature_scaler_webcode.pkl\",\n",
    "        )\n",
    "        print(f\"âœ“ Saved scaler: feature_scaler_webcode.pkl\")\n",
    "    # Save imputer (IMPORTANT!)\n",
    "    if imputer is not None:\n",
    "        joblib.dump(\n",
    "            imputer,\n",
    "            \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/feature_imputer_webcode.pkl\",\n",
    "        )\n",
    "        print(f\"âœ“ Saved imputer: feature_imputer_webcode.pkl\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"best_model\": best_model_name,\n",
    "        \"best_metrics\": results[best_model_name][\"metrics\"],\n",
    "        \"config\": CONFIG,\n",
    "        \"total_models_trained\": len(results),\n",
    "        \"imputation_strategy\": CONFIG[\"imputation_strategy\"],\n",
    "    }\n",
    "    joblib.dump(\n",
    "        metadata,\n",
    "        \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/model_metadata.pkl\",\n",
    "    )\n",
    "    print(f\"âœ“ Saved metadata: model_metadata.pkl\")\n",
    "\n",
    "    # Save feature names\n",
    "    joblib.dump(\n",
    "        feature_names,\n",
    "        \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/feature_names_webcode.pkl\",\n",
    "    )\n",
    "    print(f\"âœ“ Saved feature names: feature_names_webcode.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Imputation Strategy: ITERATIVE\n",
      "  Scaling: Enabled\n",
      "  Imbalance Handling: Enabled\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "âœ“ Dataset loaded: (80000, 81)\n",
      "\n",
      "Data Quality Check:\n",
      "  Missing values: 0 (0.00%)\n",
      "  Duplicate rows: 0\n",
      "\n",
      "Class Distribution:\n",
      "result\n",
      "0    50000\n",
      "1    30000\n",
      "Name: count, dtype: int64\n",
      "Phishing ratio: 37.50%\n",
      "Imbalance ratio: 1.67:1\n",
      "âš ï¸  Dataset is imbalanced - will apply SMOTE\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PREPARING DATA\n",
      "================================================================================\n",
      "\n",
      "Dataset Info:\n",
      "  Features: 79\n",
      "  Samples: 80000\n",
      "  Classes: 2\n",
      "\n",
      "Data Split:\n",
      "  Train set: (64000, 79)\n",
      "  Test set: (16000, 79)\n",
      "\n",
      "================================================================================\n",
      "HANDLING MISSING AND INVALID VALUES\n",
      "================================================================================\n",
      "\n",
      "2. Checking for missing values...\n",
      "   âœ“ No missing values found\n",
      "\n",
      "4. Final validation...\n",
      "   âœ“ Data is clean and ready for training\n",
      "\n",
      "5. Summary:\n",
      "   Original shape: (64000, 79)\n",
      "   Final shape: (64000, 79)\n",
      "   Features preserved: 79\n",
      "\n",
      "================================================================================\n",
      "HANDLING MISSING AND INVALID VALUES\n",
      "================================================================================\n",
      "\n",
      "2. Checking for missing values...\n",
      "   âœ“ No missing values found\n",
      "\n",
      "4. Final validation...\n",
      "   âœ“ Data is clean and ready for training\n",
      "\n",
      "5. Summary:\n",
      "   Original shape: (16000, 79)\n",
      "   Final shape: (16000, 79)\n",
      "   Features preserved: 79\n",
      "\n",
      "================================================================================\n",
      "FEATURE SCALING\n",
      "================================================================================\n",
      "\n",
      "âœ“ Scaling features with RobustScaler...\n",
      "  (RobustScaler is robust to outliers using median and IQR)\n",
      "  âœ“ Features scaled successfully\n",
      "\n",
      "================================================================================\n",
      "HANDLING CLASS IMBALANCE\n",
      "================================================================================\n",
      "\n",
      "Current class distribution:\n",
      "  Class 0 (Legit): 40,000\n",
      "  Class 1 (Phishing): 24,000\n",
      "  Imbalance ratio: 1.67:1\n",
      "\n",
      "âœ“ Applying SMOTE (Synthetic Minority Over-sampling)...\n",
      "  (Generates synthetic samples for minority class)\n",
      "\n",
      "  Results:\n",
      "    Before: 64,000 samples\n",
      "    After: 80,000 samples\n",
      "    Class 0: 40,000\n",
      "    Class 1: 40,000\n",
      "    New ratio: 1.00:1\n",
      "\n",
      "================================================================================\n",
      "STEP 3: DEFINING MODELS\n",
      "================================================================================\n",
      "âœ“ Configured 10 models:\n",
      "  â€¢ Random Forest\n",
      "  â€¢ Extra Trees\n",
      "  â€¢ Gradient Boosting\n",
      "  â€¢ XGBoost\n",
      "  â€¢ LightGBM\n",
      "  â€¢ CatBoost\n",
      "  â€¢ Naive Bayes\n",
      "  â€¢ KNN\n",
      "  â€¢ Decision Tree\n",
      "  â€¢ AdaBoost\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: Random Forest\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: Input X contains infinity or a value too large for dtype('float32').\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: Extra Trees\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: Input X contains infinity or a value too large for dtype('float32').\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: Gradient Boosting\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: Input X contains infinity or a value too large for dtype('float32').\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: XGBoost\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: [04:00:19] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n",
      "Stack trace:\n",
      "  [bt] (0) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x7f32282bdf8c]\n",
      "  [bt] (1) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x632a00) [0x7f3228632a00]\n",
      "  [bt] (2) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x64b77a) [0x7f322864b77a]\n",
      "  [bt] (3) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x64e71b) [0x7f322864e71b]\n",
      "  [bt] (4) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x64f62d) [0x7f322864f62d]\n",
      "  [bt] (5) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(+0x5c6441) [0x7f32285c6441]\n",
      "  [bt] (6) /home/maliha/Programming/dm/Phishing-Website-Classifier/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f32281ccdc8]\n",
      "  [bt] (7) /usr/lib/libffi.so.8(+0x7ac6) [0x7f32a7da2ac6]\n",
      "  [bt] (8) /usr/lib/libffi.so.8(+0x476b) [0x7f32a7d9f76b]\n",
      "\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: LightGBM\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Training completed in 1.40s\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.9671\n",
      "  Precision: 0.9562\n",
      "  Recall:    0.9560\n",
      "  F1-Score:  0.9561\n",
      "  ROC-AUC:   0.9935\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN: 9,737 | FP: 263\n",
      "  FN: 264 | TP: 5,736\n",
      "\n",
      "Error Rates:\n",
      "  FPR: 0.0263 (legit sites flagged as phishing)\n",
      "  FNR: 0.0440 (phishing sites missed)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: CatBoost\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Training completed in 20.07s\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.9702\n",
      "  Precision: 0.9630\n",
      "  Recall:    0.9573\n",
      "  F1-Score:  0.9601\n",
      "  ROC-AUC:   0.9940\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN: 9,779 | FP: 221\n",
      "  FN: 256 | TP: 5,744\n",
      "\n",
      "Error Rates:\n",
      "  FPR: 0.0221 (legit sites flagged as phishing)\n",
      "  FNR: 0.0427 (phishing sites missed)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: Naive Bayes\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Training completed in 0.22s\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.3750\n",
      "  Precision: 0.3750\n",
      "  Recall:    1.0000\n",
      "  F1-Score:  0.5455\n",
      "  ROC-AUC:   0.5000\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN: 0 | FP: 10,000\n",
      "  FN: 0 | TP: 6,000\n",
      "\n",
      "Error Rates:\n",
      "  FPR: 1.0000 (legit sites flagged as phishing)\n",
      "  FNR: 0.0000 (phishing sites missed)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: KNN\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Training completed in 0.12s\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.9333\n",
      "  Precision: 0.8983\n",
      "  Recall:    0.9270\n",
      "  F1-Score:  0.9124\n",
      "  ROC-AUC:   0.9690\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN: 9,370 | FP: 630\n",
      "  FN: 438 | TP: 5,562\n",
      "\n",
      "Error Rates:\n",
      "  FPR: 0.0630 (legit sites flagged as phishing)\n",
      "  FNR: 0.0730 (phishing sites missed)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: Decision Tree\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: Input X contains infinity or a value too large for dtype('float32').\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Training: AdaBoost\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ— Training failed: Input X contains infinity or a value too large for dtype('float32').\n",
      "\n",
      "================================================================================\n",
      "STEP 5: MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Model Performance Summary (sorted by ROC-AUC):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      Model  Accuracy  Precision  Recall  F1-Score  ROC-AUC  Train Time (s)    FPR    FNR\n",
      "   CatBoost    0.9702     0.9630  0.9573    0.9601   0.9940         20.0710 0.0221 0.0427\n",
      "   LightGBM    0.9671     0.9562  0.9560    0.9561   0.9935          1.3962 0.0263 0.0440\n",
      "        KNN    0.9333     0.8983  0.9270    0.9124   0.9690          0.1161 0.0630 0.0730\n",
      "Naive Bayes    0.3750     0.3750  1.0000    0.5455   0.5000          0.2177 1.0000 0.0000\n",
      "\n",
      "================================================================================\n",
      "ğŸ† BEST MODEL\n",
      "================================================================================\n",
      "\n",
      "Model: CatBoost\n",
      "\n",
      "Metrics:\n",
      "  âœ“ Accuracy:  0.9702 (97.02%)\n",
      "  âœ“ Precision: 0.9630 (96.30%)\n",
      "  âœ“ Recall:    0.9573 (95.73%)\n",
      "  âœ“ F1-Score:  0.9601\n",
      "  âœ“ ROC-AUC:   0.9940\n",
      "\n",
      "Confusion Matrix:\n",
      "  True Negatives:  9,779 (legit sites correctly identified)\n",
      "  False Positives: 221 (legit sites flagged as phishing)\n",
      "  False Negatives: 256 (phishing sites missed) âš ï¸\n",
      "  True Positives:  5,744 (phishing sites caught)\n",
      "\n",
      "Error Analysis:\n",
      "  FPR: 0.0221 (2.21% of legit sites flagged)\n",
      "  FNR: 0.0427 (4.27% of phishing sites missed)\n",
      "\n",
      "Speed:\n",
      "  Training: 20.07s\n",
      "  Prediction: 20.94ms\n",
      "\n",
      "================================================================================\n",
      "STEP 7: SAVING ARTIFACTS\n",
      "================================================================================\n",
      "âœ“ Saved best model: best_phishing_model_webcode.pkl\n",
      "âœ“ Saved scaler: feature_scaler_webcode.pkl\n",
      "âœ“ Saved metadata: model_metadata.pkl\n",
      "âœ“ Saved feature names: feature_names_webcode.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Imputation Strategy: {CONFIG['imputation_strategy'].upper()}\")\n",
    "print(f\"  Scaling: {'Enabled' if CONFIG['scale_data'] else 'Disabled'}\")\n",
    "print(\n",
    "    f\"  Imbalance Handling: {'Enabled' if CONFIG['handle_imbalance'] else 'Disabled'}\"\n",
    ")\n",
    "\n",
    "# Step 1: Load data\n",
    "df = load_and_explore_data(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/phishing_complete_features.csv\"\n",
    ")\n",
    "\n",
    "# Step 2: Prepare data (with advanced imputation)\n",
    "X_train, X_test, y_train, y_test, scaler, imputer, feature_names = prepare_data(\n",
    "    df, CONFIG\n",
    ")\n",
    "\n",
    "# Step 3: Define models\n",
    "models = get_models(CONFIG)\n",
    "\n",
    "# Step 4: Train all models\n",
    "results = train_all_models(models, X_train, X_test, y_train, y_test, CONFIG)\n",
    "\n",
    "# Step 5: Compare models\n",
    "best_model_name, df_comparison = compare_models(results)\n",
    "\n",
    "# Step 6: Save artifacts\n",
    "save_artifacts(best_model_name, results, scaler, imputer, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
