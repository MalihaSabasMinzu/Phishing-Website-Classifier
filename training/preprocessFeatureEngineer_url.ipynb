{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:41:08.939080Z",
     "iopub.status.busy": "2025-12-30T14:41:08.938342Z",
     "iopub.status.idle": "2025-12-30T14:42:37.616330Z",
     "shell.execute_reply": "2025-12-30T14:42:37.615478Z",
     "shell.execute_reply.started": "2025-12-30T14:41:08.939052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://intego3.info/EXEL/index.php</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.mathopenref.com/segment.html</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.computerhope.com/issues/ch000254.htm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.investopedia.com/terms/n/next-elev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jobs.emss.org.uk/lcc.aspx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  result\n",
       "0                 http://intego3.info/EXEL/index.php       1\n",
       "1           https://www.mathopenref.com/segment.html       0\n",
       "2   https://www.computerhope.com/issues/ch000254.htm       0\n",
       "3  https://www.investopedia.com/terms/n/next-elev...       0\n",
       "4                  https://jobs.emss.org.uk/lcc.aspx       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\n",
    "    \"/kaggle/input/phishing-website-webcode-dataset/phishing_complete_dataset.csv\",\n",
    "    usecols=[\"url\", \"result\"],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:42:37.618721Z",
     "iopub.status.busy": "2025-12-30T14:42:37.618323Z",
     "iopub.status.idle": "2025-12-30T14:42:37.636319Z",
     "shell.execute_reply": "2025-12-30T14:42:37.635233Z",
     "shell.execute_reply.started": "2025-12-30T14:42:37.618698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def preprocess_urls_safe(df):\n",
    "    \"\"\"\n",
    "    Minimal preprocessing that preserves all potentially useful information\n",
    "    for phishing detection feature extraction.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"URL PREPROCESSING REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "\n",
    "    # Step 1: Handle missing values\n",
    "    missing_urls = df[\"url\"].isna().sum()\n",
    "    print(f\"\\n1. Missing URLs found: {missing_urls}\")\n",
    "    if missing_urls > 0:\n",
    "        df = df.dropna(subset=[\"url\"])\n",
    "        print(f\"   Rows after removing missing URLs: {len(df)}\")\n",
    "\n",
    "    # Step 2: Handle missing labels\n",
    "    missing_labels = df[\"result\"].isna().sum()\n",
    "    print(f\"\\n2. Missing labels found: {missing_labels}\")\n",
    "    if missing_labels > 0:\n",
    "        df = df.dropna(subset=[\"result\"])\n",
    "        print(f\"   Rows after removing missing labels: {len(df)}\")\n",
    "\n",
    "    # Step 3: Strip ONLY leading/trailing whitespace (preserve internal structure)\n",
    "    df[\"url\"] = df[\"url\"].str.strip()\n",
    "    print(f\"\\n3. Whitespace stripped from URL boundaries\")\n",
    "\n",
    "    # Step 4: Check for duplicate URLs with conflicting labels\n",
    "    duplicates = df[df.duplicated(subset=[\"url\"], keep=False)]\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"\\n4. Duplicate URLs found: {len(duplicates)}\")\n",
    "\n",
    "        # Check for conflicting labels\n",
    "        conflicts = duplicates.groupby(\"url\")[\"result\"].nunique()\n",
    "        conflicting_urls = conflicts[conflicts > 1]\n",
    "\n",
    "        if len(conflicting_urls) > 0:\n",
    "            print(\n",
    "                f\"   ⚠️ WARNING: {len(conflicting_urls)} URLs have conflicting labels!\"\n",
    "            )\n",
    "            print(f\"   Conflicting URLs:\")\n",
    "            for url in conflicting_urls.index[:5]:  # Show first 5\n",
    "                labels = duplicates[duplicates[\"url\"] == url][\"result\"].unique()\n",
    "                print(f\"      {url}: labels = {labels}\")\n",
    "\n",
    "            # Strategy: Keep the majority label, or drop if tie\n",
    "            def resolve_conflict(group):\n",
    "                label_counts = group[\"result\"].value_counts()\n",
    "                if (\n",
    "                    len(label_counts) > 1\n",
    "                    and label_counts.iloc[0] == label_counts.iloc[1]\n",
    "                ):\n",
    "                    # It's a tie, drop all instances\n",
    "                    return None\n",
    "                # Keep row with most common label\n",
    "                return group[group[\"result\"] == label_counts.index[0]].iloc[0]\n",
    "\n",
    "            # Separate conflicting and non-conflicting\n",
    "            conflict_urls = conflicting_urls.index.tolist()\n",
    "            df_conflicts = df[df[\"url\"].isin(conflict_urls)]\n",
    "            df_no_conflicts = df[~df[\"url\"].isin(conflict_urls)]\n",
    "\n",
    "            # Resolve conflicts\n",
    "            resolved = (\n",
    "                df_conflicts.groupby(\"url\")\n",
    "                .apply(resolve_conflict)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            resolved = resolved.dropna()\n",
    "\n",
    "            # Combine back\n",
    "            df = pd.concat([df_no_conflicts, resolved], ignore_index=True)\n",
    "            print(f\"   Conflicts resolved. Rows after resolution: {len(df)}\")\n",
    "\n",
    "        # Remove remaining duplicates (same URL, same label)\n",
    "        df = df.drop_duplicates(subset=[\"url\"], keep=\"first\")\n",
    "        print(f\"   Duplicates removed. Final rows: {len(df)}\")\n",
    "    else:\n",
    "        print(f\"\\n4. No duplicate URLs found\")\n",
    "\n",
    "    # Step 5: Validate URL format (remove only completely unparseable URLs)\n",
    "    def is_parseable_url(url):\n",
    "        \"\"\"Check if URL can be parsed and has basic structure\"\"\"\n",
    "        try:\n",
    "            if not isinstance(url, str):\n",
    "                return False\n",
    "            if len(url.strip()) == 0:\n",
    "                return False\n",
    "            result = urlparse(url)\n",
    "            # Must have scheme (http/https/ftp etc) and netloc (domain)\n",
    "            return bool(result.scheme and result.netloc)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    df[\"is_valid\"] = df[\"url\"].apply(is_parseable_url)\n",
    "    invalid_count = (~df[\"is_valid\"]).sum()\n",
    "    print(f\"\\n5. Invalid/unparseable URLs found: {invalid_count}\")\n",
    "\n",
    "    if invalid_count > 0:\n",
    "        print(f\"   Examples of invalid URLs:\")\n",
    "        invalid_urls = df[~df[\"is_valid\"]][\"url\"].head(5).tolist()\n",
    "        for invalid_url in invalid_urls:\n",
    "            print(f\"      {invalid_url}\")\n",
    "\n",
    "    df = df[df[\"is_valid\"]].drop(\"is_valid\", axis=1)\n",
    "\n",
    "    # Step 6: Ensure labels are in correct format (0 and 1)\n",
    "    unique_labels = df[\"result\"].unique()\n",
    "    print(f\"\\n6. Label distribution:\")\n",
    "    print(df[\"result\"].value_counts().to_dict())\n",
    "\n",
    "    if not all(label in [0, 1] for label in unique_labels):\n",
    "        print(\n",
    "            f\"   ⚠️ WARNING: Labels contain values other than 0 and 1: {unique_labels}\"\n",
    "        )\n",
    "        print(f\"   Converting labels to binary (0/1)...\")\n",
    "        df[\"result\"] = df[\"result\"].astype(int)\n",
    "\n",
    "    # Step 7: Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PREPROCESSING COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Final dataset size: {len(df)}\")\n",
    "    print(f\"Legitimate URLs (0): {(df['result'] == 0).sum()}\")\n",
    "    print(f\"Phishing URLs (1): {(df['result'] == 1).sum()}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:42:37.637866Z",
     "iopub.status.busy": "2025-12-30T14:42:37.637443Z",
     "iopub.status.idle": "2025-12-30T14:42:38.407968Z",
     "shell.execute_reply": "2025-12-30T14:42:38.406463Z",
     "shell.execute_reply.started": "2025-12-30T14:42:37.637831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "URL PREPROCESSING REPORT\n",
      "==================================================\n",
      "Initial dataset size: 80000\n",
      "\n",
      "1. Missing URLs found: 0\n",
      "\n",
      "2. Missing labels found: 0\n",
      "\n",
      "3. Whitespace stripped from URL boundaries\n",
      "\n",
      "4. Duplicate URLs found: 286\n",
      "   ⚠️ WARNING: 1 URLs have conflicting labels!\n",
      "   Conflicting URLs:\n",
      "      https://www.meresearch.org.uk/what-is-me/: labels = [0 1]\n",
      "   Conflicts resolved. Rows after resolution: 79998\n",
      "   Duplicates removed. Final rows: 79847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/4176480222.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  resolved = df_conflicts.groupby('url').apply(resolve_conflict).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Invalid/unparseable URLs found: 0\n",
      "\n",
      "6. Label distribution:\n",
      "{0: 49853, 1: 29994}\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING COMPLETE\n",
      "==================================================\n",
      "Final dataset size: 79847\n",
      "Legitimate URLs (0): 49853\n",
      "Phishing URLs (1): 29994\n",
      "==================================================\n",
      "\n",
      "✅ Cleaned dataset saved to: /kaggle/working/cleaned_urls_dataset.csv\n",
      "\n",
      "First 5 rows of cleaned dataset:\n",
      "                                                 url  result\n",
      "0                 http://intego3.info/EXEL/index.php       1\n",
      "1           https://www.mathopenref.com/segment.html       0\n",
      "2   https://www.computerhope.com/issues/ch000254.htm       0\n",
      "3  https://www.investopedia.com/terms/n/next-elev...       0\n",
      "4                  https://jobs.emss.org.uk/lcc.aspx       0\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing\n",
    "df_clean = preprocess_urls_safe(df)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_filename = \"/kaggle/working/cleaned_urls_dataset.csv\"\n",
    "df_clean.to_csv(output_filename, index=False)\n",
    "print(f\"✅ Cleaned dataset saved to: {output_filename}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of cleaned dataset:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "from math import log2\n",
    "import tldextract\n",
    "\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of a string\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    # Count frequency of each character\n",
    "    prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -sum([p * log2(p) for p in prob])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def has_ip_address(url):\n",
    "    \"\"\"Check if URL contains an IP address instead of domain name\"\"\"\n",
    "    # IPv4 pattern\n",
    "    ipv4_pattern = re.compile(r\"(\\d{1,3}\\.){3}\\d{1,3}\")\n",
    "    # IPv6 pattern (simplified)\n",
    "    ipv6_pattern = re.compile(\n",
    "        r\"(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4})\"\n",
    "    )\n",
    "\n",
    "    return bool(ipv4_pattern.search(url) or ipv6_pattern.search(url))\n",
    "\n",
    "\n",
    "def count_suspicious_keywords(text):\n",
    "    \"\"\"Count suspicious keywords commonly used in phishing\"\"\"\n",
    "    suspicious_words = [\n",
    "        \"verify\",\n",
    "        \"account\",\n",
    "        \"update\",\n",
    "        \"secure\",\n",
    "        \"banking\",\n",
    "        \"login\",\n",
    "        \"signin\",\n",
    "        \"ebayisapi\",\n",
    "        \"webscr\",\n",
    "        \"password\",\n",
    "        \"confirm\",\n",
    "        \"suspend\",\n",
    "        \"alert\",\n",
    "        \"authenticate\",\n",
    "        \"wallet\",\n",
    "        \"credential\",\n",
    "        \"security\",\n",
    "        \"urgent\",\n",
    "    ]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    count = sum(1 for word in suspicious_words if word in text_lower)\n",
    "    return count\n",
    "\n",
    "\n",
    "def has_shortening_service(url):\n",
    "    \"\"\"Check if URL uses URL shortening service\"\"\"\n",
    "    shortening_services = [\n",
    "        \"bit.ly\",\n",
    "        \"goo.gl\",\n",
    "        \"tinyurl\",\n",
    "        \"t.co\",\n",
    "        \"ow.ly\",\n",
    "        \"is.gd\",\n",
    "        \"buff.ly\",\n",
    "        \"adf.ly\",\n",
    "        \"bit.do\",\n",
    "        \"short.link\",\n",
    "        \"tiny.cc\",\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    return int(any(service in url_lower for service in shortening_services))\n",
    "\n",
    "\n",
    "def get_tld_type(domain):\n",
    "    \"\"\"Classify TLD as common, suspicious, or other\"\"\"\n",
    "    common_tlds = [\".com\", \".org\", \".net\", \".edu\", \".gov\", \".co\", \".uk\", \".de\", \".fr\"]\n",
    "    suspicious_tlds = [\".tk\", \".ml\", \".ga\", \".cf\", \".gq\", \".pw\", \".cc\", \".club\", \".xyz\"]\n",
    "\n",
    "    domain_lower = domain.lower()\n",
    "\n",
    "    if any(domain_lower.endswith(tld) for tld in common_tlds):\n",
    "        return 1  # Common TLD\n",
    "    elif any(domain_lower.endswith(tld) for tld in suspicious_tlds):\n",
    "        return 2  # Suspicious TLD\n",
    "    else:\n",
    "        return 0  # Other TLD\n",
    "\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract all features from a single URL\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        # Parse URL\n",
    "        parsed = urlparse(url)\n",
    "\n",
    "        # Extract domain components using tldextract\n",
    "        ext = tldextract.extract(url)\n",
    "        domain = ext.domain\n",
    "        subdomain = ext.subdomain\n",
    "        suffix = ext.suffix\n",
    "        full_domain = f\"{domain}.{suffix}\" if suffix else domain\n",
    "\n",
    "        # ======================\n",
    "        # 1. URL LENGTH FEATURES\n",
    "        # ======================\n",
    "        features[\"url_length\"] = len(url)\n",
    "        features[\"domain_length\"] = len(full_domain) if full_domain else 0\n",
    "        features[\"path_length\"] = len(parsed.path)\n",
    "        features[\"query_length\"] = len(parsed.query)\n",
    "        features[\"fragment_length\"] = len(parsed.fragment)\n",
    "\n",
    "        # Count subdirectories in path\n",
    "        path_parts = [p for p in parsed.path.split(\"/\") if p]\n",
    "        features[\"subdirectory_count\"] = len(path_parts)\n",
    "\n",
    "        # ======================\n",
    "        # 2. PROTOCOL FEATURES\n",
    "        # ======================\n",
    "        features[\"has_https\"] = int(parsed.scheme == \"https\")\n",
    "        features[\"has_http\"] = int(parsed.scheme == \"http\")\n",
    "\n",
    "        # ======================\n",
    "        # 3. DOMAIN FEATURES\n",
    "        # ======================\n",
    "        features[\"dot_count_in_domain\"] = full_domain.count(\".\")\n",
    "        features[\"subdomain_count\"] = len(subdomain.split(\".\")) if subdomain else 0\n",
    "        features[\"has_subdomain\"] = int(bool(subdomain))\n",
    "        features[\"has_ip_address\"] = int(has_ip_address(url))\n",
    "        features[\"domain_has_numbers\"] = int(bool(re.search(r\"\\d\", full_domain)))\n",
    "        features[\"suspicious_keywords_in_domain\"] = count_suspicious_keywords(\n",
    "            full_domain\n",
    "        )\n",
    "\n",
    "        # ======================\n",
    "        # 4. SPECIAL CHARACTER FEATURES\n",
    "        # ======================\n",
    "        features[\"at_symbol_count\"] = url.count(\"@\")\n",
    "        features[\"hyphen_count\"] = url.count(\"-\")\n",
    "        features[\"underscore_count\"] = url.count(\"_\")\n",
    "        features[\"question_mark_count\"] = url.count(\"?\")\n",
    "        features[\"equal_count\"] = url.count(\"=\")\n",
    "        features[\"ampersand_count\"] = url.count(\"&\")\n",
    "        features[\"percent_count\"] = url.count(\"%\")\n",
    "        features[\"double_slash_count\"] = (\n",
    "            url.count(\"//\") - 1\n",
    "        )  # Subtract the one in http://\n",
    "        features[\"digit_count\"] = sum(c.isdigit() for c in url)\n",
    "        features[\"letter_count\"] = sum(c.isalpha() for c in url)\n",
    "        features[\"dot_count\"] = url.count(\".\")\n",
    "        features[\"slash_count\"] = url.count(\"/\")\n",
    "\n",
    "        # ======================\n",
    "        # 5. PATH FEATURES\n",
    "        # ======================\n",
    "        features[\"path_depth\"] = len(path_parts)\n",
    "\n",
    "        # File extension\n",
    "        if path_parts:\n",
    "            last_part = path_parts[-1]\n",
    "            if \".\" in last_part:\n",
    "                extension = last_part.split(\".\")[-1].lower()\n",
    "                features[\"has_file_extension\"] = 1\n",
    "\n",
    "                # Suspicious extensions\n",
    "                suspicious_extensions = [\"exe\", \"zip\", \"rar\", \"php\", \"js\", \"bin\", \"scr\"]\n",
    "                features[\"has_suspicious_extension\"] = int(\n",
    "                    extension in suspicious_extensions\n",
    "                )\n",
    "            else:\n",
    "                features[\"has_file_extension\"] = 0\n",
    "                features[\"has_suspicious_extension\"] = 0\n",
    "        else:\n",
    "            features[\"has_file_extension\"] = 0\n",
    "            features[\"has_suspicious_extension\"] = 0\n",
    "\n",
    "        # ======================\n",
    "        # 6. SUSPICIOUS PATTERN FEATURES\n",
    "        # ======================\n",
    "        features[\"has_at_symbol\"] = int(\"@\" in url)\n",
    "        features[\"has_port\"] = int(bool(parsed.port))\n",
    "        features[\"has_shortening_service\"] = has_shortening_service(url)\n",
    "        features[\"suspicious_keywords_in_url\"] = count_suspicious_keywords(url)\n",
    "\n",
    "        # Check for hexadecimal characters (common in encoded URLs)\n",
    "        hex_pattern = re.compile(r\"%[0-9a-fA-F]{2}\")\n",
    "        features[\"has_hex_encoding\"] = int(bool(hex_pattern.search(url)))\n",
    "\n",
    "        # Prefix/suffix hyphen in domain\n",
    "        features[\"prefix_suffix_hyphen\"] = int(\"-\" in full_domain)\n",
    "\n",
    "        # ======================\n",
    "        # 7. ENTROPY FEATURES\n",
    "        # ======================\n",
    "        features[\"url_entropy\"] = calculate_entropy(url)\n",
    "        features[\"domain_entropy\"] = calculate_entropy(full_domain)\n",
    "        features[\"path_entropy\"] = calculate_entropy(parsed.path) if parsed.path else 0\n",
    "\n",
    "        # ======================\n",
    "        # 8. CHARACTER TYPE RATIOS\n",
    "        # ======================\n",
    "        url_len = len(url) if len(url) > 0 else 1  # Avoid division by zero\n",
    "        features[\"digit_ratio\"] = features[\"digit_count\"] / url_len\n",
    "        features[\"letter_ratio\"] = features[\"letter_count\"] / url_len\n",
    "\n",
    "        # Count special characters\n",
    "        special_char_count = sum(not c.isalnum() for c in url)\n",
    "        features[\"special_char_ratio\"] = special_char_count / url_len\n",
    "\n",
    "        # Uppercase to lowercase ratio\n",
    "        uppercase_count = sum(c.isupper() for c in url)\n",
    "        lowercase_count = sum(c.islower() for c in url)\n",
    "        total_letters = uppercase_count + lowercase_count\n",
    "        features[\"uppercase_ratio\"] = (\n",
    "            uppercase_count / total_letters if total_letters > 0 else 0\n",
    "        )\n",
    "\n",
    "        # ======================\n",
    "        # 9. LEXICAL FEATURES\n",
    "        # ======================\n",
    "        # Split URL into words (by non-alphanumeric characters)\n",
    "        words = re.findall(r\"\\b\\w+\\b\", url)\n",
    "        features[\"word_count\"] = len(words)\n",
    "\n",
    "        # Average word length\n",
    "        if words:\n",
    "            features[\"avg_word_length\"] = np.mean([len(word) for word in words])\n",
    "            features[\"longest_word_length\"] = max([len(word) for word in words])\n",
    "        else:\n",
    "            features[\"avg_word_length\"] = 0\n",
    "            features[\"longest_word_length\"] = 0\n",
    "\n",
    "        # TLD classification\n",
    "        features[\"tld_type\"] = get_tld_type(full_domain)\n",
    "\n",
    "        # ======================\n",
    "        # 10. REDIRECTION FEATURES\n",
    "        # ======================\n",
    "        # Multiple // in URL (excluding the one in protocol)\n",
    "        features[\"multiple_redirects\"] = int(url.count(\"//\") > 1)\n",
    "\n",
    "        # Query parameters count\n",
    "        if parsed.query:\n",
    "            query_params = parse_qs(parsed.query)\n",
    "            features[\"query_param_count\"] = len(query_params)\n",
    "        else:\n",
    "            features[\"query_param_count\"] = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL: {url}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # Return features with default values (0 or NaN)\n",
    "        return {key: 0 for key in features.keys()}\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Apply feature engineering to the entire dataset\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with 'url' and 'result' columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with all engineered features\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Processing {len(df)} URLs...\")\n",
    "\n",
    "    # Extract features for all URLs\n",
    "    features_list = []\n",
    "\n",
    "    for idx, url in enumerate(df[\"url\"]):\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(df)} URLs...\")\n",
    "\n",
    "        features = extract_url_features(url)\n",
    "        features[\"url\"] = url  # Keep original URL\n",
    "        features[\"result\"] = df.loc[idx, \"result\"]  # Keep label\n",
    "        features_list.append(features)\n",
    "\n",
    "    # Create DataFrame from features\n",
    "    df_features = pd.DataFrame(features_list)\n",
    "\n",
    "    # Reorder columns: url, result, then features\n",
    "    feature_cols = [col for col in df_features.columns if col not in [\"url\", \"result\"]]\n",
    "    df_features = df_features[[\"url\", \"result\"] + feature_cols]\n",
    "\n",
    "    print(f\"\\n✅ Feature engineering complete!\")\n",
    "    print(f\"Total features extracted: {len(feature_cols)}\")\n",
    "    print(f\"\\nFeature columns:\")\n",
    "    for i, col in enumerate(feature_cols, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset shape: {df_features.shape}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset...\n",
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "Processing 79847 URLs...\n",
      "Processed 1000/79847 URLs...\n",
      "Processed 2000/79847 URLs...\n",
      "Processed 3000/79847 URLs...\n",
      "Processed 4000/79847 URLs...\n",
      "Processed 5000/79847 URLs...\n",
      "Processed 6000/79847 URLs...\n",
      "Processed 7000/79847 URLs...\n",
      "Processed 8000/79847 URLs...\n",
      "Processed 9000/79847 URLs...\n",
      "Processed 10000/79847 URLs...\n",
      "Processed 11000/79847 URLs...\n",
      "Processed 12000/79847 URLs...\n",
      "Processed 13000/79847 URLs...\n",
      "Processed 14000/79847 URLs...\n",
      "Processed 15000/79847 URLs...\n",
      "Processed 16000/79847 URLs...\n",
      "Processed 17000/79847 URLs...\n",
      "Processed 18000/79847 URLs...\n",
      "Processed 19000/79847 URLs...\n",
      "Processed 20000/79847 URLs...\n",
      "Processed 21000/79847 URLs...\n",
      "Processed 22000/79847 URLs...\n",
      "Processed 23000/79847 URLs...\n",
      "Processed 24000/79847 URLs...\n",
      "Processed 25000/79847 URLs...\n",
      "Processed 26000/79847 URLs...\n",
      "Processed 27000/79847 URLs...\n",
      "Processed 28000/79847 URLs...\n",
      "Processed 29000/79847 URLs...\n",
      "Processed 30000/79847 URLs...\n",
      "Processed 31000/79847 URLs...\n",
      "Processed 32000/79847 URLs...\n",
      "Processed 33000/79847 URLs...\n",
      "Processed 34000/79847 URLs...\n",
      "Processed 35000/79847 URLs...\n",
      "Processed 36000/79847 URLs...\n",
      "Processed 37000/79847 URLs...\n",
      "Processed 38000/79847 URLs...\n",
      "Processed 39000/79847 URLs...\n",
      "Processed 40000/79847 URLs...\n",
      "Processed 41000/79847 URLs...\n",
      "Processed 42000/79847 URLs...\n",
      "Processed 43000/79847 URLs...\n",
      "Processed 44000/79847 URLs...\n",
      "Processed 45000/79847 URLs...\n",
      "Processed 46000/79847 URLs...\n",
      "Processed 47000/79847 URLs...\n",
      "Processed 48000/79847 URLs...\n",
      "Processed 49000/79847 URLs...\n",
      "Processed 50000/79847 URLs...\n",
      "Processed 51000/79847 URLs...\n",
      "Processed 52000/79847 URLs...\n",
      "Processed 53000/79847 URLs...\n",
      "Processed 54000/79847 URLs...\n",
      "Processed 55000/79847 URLs...\n",
      "Processed 56000/79847 URLs...\n",
      "Processed 57000/79847 URLs...\n",
      "Processed 58000/79847 URLs...\n",
      "Processed 59000/79847 URLs...\n",
      "Processed 60000/79847 URLs...\n",
      "Processed 61000/79847 URLs...\n",
      "Processed 62000/79847 URLs...\n",
      "Processed 63000/79847 URLs...\n",
      "Processed 64000/79847 URLs...\n",
      "Processed 65000/79847 URLs...\n",
      "Processed 66000/79847 URLs...\n",
      "Processed 67000/79847 URLs...\n",
      "Processed 68000/79847 URLs...\n",
      "Processed 69000/79847 URLs...\n",
      "Processed 70000/79847 URLs...\n",
      "Processed 71000/79847 URLs...\n",
      "Processed 72000/79847 URLs...\n",
      "Processed 73000/79847 URLs...\n",
      "Processed 74000/79847 URLs...\n",
      "Processed 75000/79847 URLs...\n",
      "Processed 76000/79847 URLs...\n",
      "Processed 77000/79847 URLs...\n",
      "Processed 78000/79847 URLs...\n",
      "Processed 79000/79847 URLs...\n",
      "\n",
      "✅ Feature engineering complete!\n",
      "Total features extracted: 48\n",
      "\n",
      "Feature columns:\n",
      "  1. url_length\n",
      "  2. domain_length\n",
      "  3. path_length\n",
      "  4. query_length\n",
      "  5. fragment_length\n",
      "  6. subdirectory_count\n",
      "  7. has_https\n",
      "  8. has_http\n",
      "  9. dot_count_in_domain\n",
      "  10. subdomain_count\n",
      "  11. has_subdomain\n",
      "  12. has_ip_address\n",
      "  13. domain_has_numbers\n",
      "  14. suspicious_keywords_in_domain\n",
      "  15. at_symbol_count\n",
      "  16. hyphen_count\n",
      "  17. underscore_count\n",
      "  18. question_mark_count\n",
      "  19. equal_count\n",
      "  20. ampersand_count\n",
      "  21. percent_count\n",
      "  22. double_slash_count\n",
      "  23. digit_count\n",
      "  24. letter_count\n",
      "  25. dot_count\n",
      "  26. slash_count\n",
      "  27. path_depth\n",
      "  28. has_file_extension\n",
      "  29. has_suspicious_extension\n",
      "  30. has_at_symbol\n",
      "  31. has_port\n",
      "  32. has_shortening_service\n",
      "  33. suspicious_keywords_in_url\n",
      "  34. has_hex_encoding\n",
      "  35. prefix_suffix_hyphen\n",
      "  36. url_entropy\n",
      "  37. domain_entropy\n",
      "  38. path_entropy\n",
      "  39. digit_ratio\n",
      "  40. letter_ratio\n",
      "  41. special_char_ratio\n",
      "  42. uppercase_ratio\n",
      "  43. word_count\n",
      "  44. avg_word_length\n",
      "  45. longest_word_length\n",
      "  46. tld_type\n",
      "  47. multiple_redirects\n",
      "  48. query_param_count\n",
      "\n",
      "============================================================\n",
      "Dataset shape: (79847, 50)\n",
      "============================================================\n",
      "\n",
      "✅ Feature-engineered dataset saved to: /home/maliha/Programming/dm/Phishing-Website-Classifier/url_features_dataset.csv\n",
      "\n",
      "============================================================\n",
      "FEATURE STATISTICS\n",
      "============================================================\n",
      "             result    url_length  domain_length   path_length  query_length  \\\n",
      "count  79847.000000  79847.000000   79847.000000  79847.000000  79847.000000   \n",
      "mean       0.375643     63.923153      14.575188     27.138891      8.760755   \n",
      "std        0.484292     52.507597       5.256974     27.611524     43.179199   \n",
      "min        0.000000     13.000000       4.000000      0.000000      0.000000   \n",
      "25%        0.000000     38.000000      11.000000      8.000000      0.000000   \n",
      "50%        0.000000     51.000000      14.000000     20.000000      0.000000   \n",
      "75%        1.000000     73.000000      17.000000     39.000000      0.000000   \n",
      "max        1.000000   1641.000000      68.000000    916.000000   1585.000000   \n",
      "\n",
      "       fragment_length  subdirectory_count     has_https      has_http  \\\n",
      "count     79847.000000        79847.000000  79847.000000  79847.000000   \n",
      "mean          0.096735            1.961101      0.766203      0.233797   \n",
      "std           2.869182            1.711901      0.423247      0.423247   \n",
      "min           0.000000            0.000000      0.000000      0.000000   \n",
      "25%           0.000000            1.000000      1.000000      0.000000   \n",
      "50%           0.000000            2.000000      1.000000      0.000000   \n",
      "75%           0.000000            3.000000      1.000000      0.000000   \n",
      "max         486.000000           28.000000      1.000000      1.000000   \n",
      "\n",
      "       dot_count_in_domain  ...   digit_ratio  letter_ratio  \\\n",
      "count         79847.000000  ...  79847.000000  79847.000000   \n",
      "mean              1.076521  ...      0.048557      0.771354   \n",
      "std               0.288472  ...      0.087147      0.082223   \n",
      "min               1.000000  ...      0.000000      0.023356   \n",
      "25%               1.000000  ...      0.000000      0.741935   \n",
      "50%               1.000000  ...      0.000000      0.794118   \n",
      "75%               1.000000  ...      0.068182      0.826087   \n",
      "max               3.000000  ...      0.970498      0.960432   \n",
      "\n",
      "       special_char_ratio  uppercase_ratio    word_count  avg_word_length  \\\n",
      "count        79847.000000     79847.000000  79847.000000     79847.000000   \n",
      "mean             0.180089         0.022377      8.666738         6.121048   \n",
      "std              0.040299         0.063198      5.528559         2.640093   \n",
      "min              0.006146         0.000000      3.000000         1.888889   \n",
      "25%              0.156863         0.000000      5.000000         4.875000   \n",
      "50%              0.177778         0.000000      7.000000         5.666667   \n",
      "75%              0.200000         0.000000     10.000000         6.666667   \n",
      "max              0.583333         0.788991    209.000000       202.125000   \n",
      "\n",
      "       longest_word_length      tld_type  multiple_redirects  \\\n",
      "count         79847.000000  79847.000000        79847.000000   \n",
      "mean             15.350984      0.844653            0.005698   \n",
      "std              19.890074      0.421759            0.075273   \n",
      "min               4.000000      0.000000            0.000000   \n",
      "25%               9.000000      1.000000            0.000000   \n",
      "50%              12.000000      1.000000            0.000000   \n",
      "75%              15.000000      1.000000            0.000000   \n",
      "max            1568.000000      2.000000            1.000000   \n",
      "\n",
      "       query_param_count  \n",
      "count       79847.000000  \n",
      "mean            0.199870  \n",
      "std             0.802063  \n",
      "min             0.000000  \n",
      "25%             0.000000  \n",
      "50%             0.000000  \n",
      "75%             0.000000  \n",
      "max            51.000000  \n",
      "\n",
      "[8 rows x 49 columns]\n",
      "\n",
      "============================================================\n",
      "SAMPLE DATA (first 3 rows)\n",
      "============================================================\n",
      "                                                url  result  url_length  \\\n",
      "0                http://intego3.info/EXEL/index.php       1          34   \n",
      "1          https://www.mathopenref.com/segment.html       0          40   \n",
      "2  https://www.computerhope.com/issues/ch000254.htm       0          48   \n",
      "\n",
      "   domain_length  path_length  query_length  fragment_length  \\\n",
      "0             12           15             0                0   \n",
      "1             15           13             0                0   \n",
      "2             16           20             0                0   \n",
      "\n",
      "   subdirectory_count  has_https  has_http  ...  digit_ratio  letter_ratio  \\\n",
      "0                   2          0         1  ...     0.029412      0.764706   \n",
      "1                   1          1         0  ...     0.000000      0.825000   \n",
      "2                   2          1         0  ...     0.125000      0.708333   \n",
      "\n",
      "   special_char_ratio  uppercase_ratio  word_count  avg_word_length  \\\n",
      "0            0.205882         0.153846           6         4.500000   \n",
      "1            0.175000         0.000000           6         5.500000   \n",
      "2            0.166667         0.000000           7         5.714286   \n",
      "\n",
      "   longest_word_length  tld_type  multiple_redirects  query_param_count  \n",
      "0                    7         0                   0                  0  \n",
      "1                   11         1                   0                  0  \n",
      "2                   12         1                   0                  0  \n",
      "\n",
      "[3 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df_clean = pd.read_csv(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/cleaned_urls_dataset.csv\"\n",
    ")\n",
    "\n",
    "# Perform feature engineering\n",
    "df_features = engineer_features(df_clean)\n",
    "\n",
    "# Save feature-engineered dataset\n",
    "output_filename = (\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/url_features_dataset.csv\"\n",
    ")\n",
    "df_features.to_csv(output_filename, index=False)\n",
    "print(f\"✅ Feature-engineered dataset saved to: {output_filename}\")\n",
    "\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(df_features.describe())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE DATA (first 3 rows)\")\n",
    "print(\"=\" * 60)\n",
    "print(df_features.head(3))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8021436,
     "sourceId": 12692784,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
