{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from web_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T17:37:43.320012Z",
     "iopub.status.busy": "2025-08-09T17:37:43.319204Z",
     "iopub.status.idle": "2025-08-09T17:37:52.206553Z",
     "shell.execute_reply": "2025-08-09T17:37:52.205494Z",
     "shell.execute_reply.started": "2025-08-09T17:37:43.319962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T17:37:52.209431Z",
     "iopub.status.busy": "2025-08-09T17:37:52.208489Z",
     "iopub.status.idle": "2025-08-09T17:41:18.021983Z",
     "shell.execute_reply": "2025-08-09T17:41:18.021016Z",
     "shell.execute_reply.started": "2025-08-09T17:37:52.209397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    \"/kaggle/input/phishing-website-webcode-dataset/phishing_complete_dataset.csv\",\n",
    "    sep=\",\",\n",
    "    quotechar='\"',\n",
    ")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T17:41:18.023164Z",
     "iopub.status.busy": "2025-08-09T17:41:18.022849Z",
     "iopub.status.idle": "2025-08-09T17:41:18.160446Z",
     "shell.execute_reply": "2025-08-09T17:41:18.159576Z",
     "shell.execute_reply.started": "2025-08-09T17:41:18.023140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check dataset shape and class distribution\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(dataset[\"result\"].value_counts())\n",
    "print(f\"\\nClass distribution (percentages):\")\n",
    "print(dataset[\"result\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Check length of webpage_code\n",
    "dataset[\"code_length\"] = dataset[\"webpage_code\"].str.len()\n",
    "print(f\"\\nWebpage code length statistics:\")\n",
    "print(dataset[\"code_length\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T17:41:18.162046Z",
     "iopub.status.busy": "2025-08-09T17:41:18.161788Z",
     "iopub.status.idle": "2025-08-09T17:41:18.219485Z",
     "shell.execute_reply": "2025-08-09T17:41:18.218595Z",
     "shell.execute_reply.started": "2025-08-09T17:41:18.162026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Features (X) = webpage_code, Target (y) = result\n",
    "X = dataset[\"webpage_code\"]\n",
    "y = dataset[\"result\"]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "del X\n",
    "del y\n",
    "del dataset\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test target distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T17:41:18.221900Z",
     "iopub.status.busy": "2025-08-09T17:41:18.221575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vectorize once for both TF-IDF and CountVectorizer, then release raw data\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, stop_words=\"english\", ngram_range=(1, 2)\n",
    ")\n",
    "# count_vectorizer = CountVectorizer(\n",
    "#     max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "# X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Release raw text data from memory\n",
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "ARTIFACT_DIR = \"/kaggle/working/\"\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(ARTIFACT_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "save_npz(os.path.join(ARTIFACT_DIR, \"X_train_tfidf.npz\"), X_train_tfidf)\n",
    "save_npz(os.path.join(ARTIFACT_DIR, \"X_test_tfidf.npz\"), X_test_tfidf)\n",
    "\n",
    "y_train_np = y_train.to_numpy() if hasattr(y_train, \"to_numpy\") else np.asarray(y_train)\n",
    "y_test_np = y_test.to_numpy() if hasattr(y_test, \"to_numpy\") else np.asarray(y_test)\n",
    "np.save(os.path.join(ARTIFACT_DIR, \"y_train.npy\"), y_train_np)\n",
    "np.save(os.path.join(ARTIFACT_DIR, \"y_test.npy\"), y_test_np)\n",
    "\n",
    "# joblib.dump(count_vectorizer, os.path.join(\n",
    "#     ARTIFACT_DIR, \"count_vectorizer.joblib\"))\n",
    "\n",
    "# save_npz(os.path.join(ARTIFACT_DIR, \"X_train_count.npz\"), X_train_count)\n",
    "# save_npz(os.path.join(ARTIFACT_DIR, \"X_test_count.npz\"), X_test_count)\n",
    "\n",
    "# np.save(os.path.join(ARTIFACT_DIR, \"y_train.npy\"), y_train_np)\n",
    "# np.save(os.path.join(ARTIFACT_DIR, \"y_test.npy\"), y_test_np)\n",
    "\n",
    "print(f\"Saved TF-IDF vectorizer and datasets to '{ARTIFACT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "classifiers_tfidf = {\n",
    "    \"Random Forest (TF-IDF)\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost (TF-IDF)\": xgb.XGBClassifier(random_state=42, verbosity=0),\n",
    "    \"LightGBM (TF-IDF)\": lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "    \"Extra Trees (TF-IDF)\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "# classifiers_count = {\n",
    "#     'Random Forest (Count)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     'XGBoost (Count)': xgb.XGBClassifier(random_state=42, verbosity=0),\n",
    "#     'LightGBM (Count)': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "#     'Extra Trees (Count)': ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "# }\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train TF-IDF models\n",
    "for name, clf in classifiers_tfidf.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        clf.fit(X_train_tfidf, y_train)\n",
    "        y_pred = clf.predict(X_test_tfidf)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1,\n",
    "            \"predictions\": y_pred,\n",
    "            \"model\": clf,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "\n",
    "# Train CountVectorizer models\n",
    "# for name, clf in classifiers_count.items():\n",
    "#     print(f\"\\nTraining {name}...\")\n",
    "#     try:\n",
    "#         clf.fit(X_train_count, y_train)\n",
    "#         y_pred = clf.predict(X_test_count)\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         f1 = f1_score(y_test, y_pred)\n",
    "#         results[name] = {\n",
    "#             'accuracy': accuracy,\n",
    "#             'f1_score': f1,\n",
    "#             'predictions': y_pred,\n",
    "#             'model': clf\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error training {name}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    comparison_data.append(\n",
    "        {\"Model\": name, \"Accuracy\": result[\"accuracy\"], \"F1 Score\": result[\"f1_score\"]}\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values(\"Accuracy\", ascending=False)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = comparison_df.iloc[0][\"Model\"]\n",
    "best_model = results[best_model_name][\"model\"]\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(comparison_df)), comparison_df[\"Accuracy\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.xticks(\n",
    "    range(len(comparison_df)), comparison_df[\"Model\"].tolist(), rotation=45, ha=\"right\"\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot 2: F1 Score comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(comparison_df)), comparison_df[\"F1 Score\"], color=\"lightcoral\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Model F1 Score Comparison\")\n",
    "plt.xticks(\n",
    "    range(len(comparison_df)), comparison_df[\"Model\"].tolist(), rotation=45, ha=\"right\"\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix for the best model\n",
    "best_predictions = results[best_model_name][\"predictions\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Legitimate\", \"Phishing\"],\n",
    "    yticklabels=[\"Legitimate\", \"Phishing\"],\n",
    ")\n",
    "plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to predict if a webpage is phishing or legitimate\n",
    "def predict_webpage_status(webpage_code, model=None):\n",
    "    \"\"\"\n",
    "    Predict if a webpage is phishing (1) or legitimate (0) based on its HTML code.\n",
    "\n",
    "    Parameters:\n",
    "    webpage_code (str): The HTML code of the webpage\n",
    "    model: The trained model to use for prediction (default: best model)\n",
    "\n",
    "    Returns:\n",
    "    dict: Prediction result with probability scores\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = best_model\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict([webpage_code])[0]\n",
    "\n",
    "    # Get prediction probabilities\n",
    "    probabilities = model.predict_proba([webpage_code])[0]\n",
    "\n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        \"prediction\": prediction,\n",
    "        \"status\": \"Phishing\" if prediction == 1 else \"Legitimate\",\n",
    "        \"confidence\": max(probabilities),\n",
    "        \"probability_legitimate\": probabilities[0],\n",
    "        \"probability_phishing\": probabilities[1],\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test the function with a sample from the test set\n",
    "# sample_index = 22\n",
    "# sample_code = X_train_tfidf.iloc[sample_index]\n",
    "# actual_label = y_test.iloc[sample_index]\n",
    "\n",
    "# prediction_result = predict_webpage_status(sample_code)\n",
    "\n",
    "# print(\"Testing the prediction function:\")\n",
    "# print(\"=\" * 40)\n",
    "# print(\n",
    "#     f\"Actual label: {actual_label} ({'Phishing' if actual_label == 1 else 'Legitimate'})\")\n",
    "# print(f\"Predicted: {prediction_result['status']}\")\n",
    "# print(f\"Confidence: {prediction_result['confidence']:.4f}\")\n",
    "# print(\n",
    "#     f\"Probability Legitimate: {prediction_result['probability_legitimate']:.4f}\")\n",
    "# print(f\"Probability Phishing: {prediction_result['probability_phishing']:.4f}\")\n",
    "\n",
    "# # Test with a few more samples\n",
    "# print(f\"\\nTesting with 5 random samples:\")\n",
    "# print(\"=\" * 50)\n",
    "# for i in range(5):\n",
    "#     sample_code = X_train_tfidf.iloc[i]\n",
    "#     actual_label = y_test.iloc[i]\n",
    "#     prediction_result = predict_webpage_status(sample_code)\n",
    "\n",
    "#     correct = \"✓\" if prediction_result['prediction'] == actual_label else \"✗\"\n",
    "#     print(f\"Sample {i+1}: Actual: {actual_label}, Predicted: {prediction_result['prediction']}, \"\n",
    "#           f\"Confidence: {prediction_result['confidence']:.3f} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing URL Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "url_dataSet = pd.read_csv(\"/kaggle/input/phising-website-url-dataset/new_data_urls.csv\")\n",
    "url_dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare features and target for URL dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url_X = url_dataSet[\"url\"]\n",
    "# fallback if column name differs\n",
    "url_y = url_dataSet[\"status\"]\n",
    "\n",
    "print(f\"URL dataset shape: {url_dataSet.shape}\")\n",
    "print(f\"Class distribution:\\n{url_y.value_counts()}\")\n",
    "\n",
    "# Split into train/test sets\n",
    "url_X_train, url_X_test, url_y_train, url_y_test = train_test_split(\n",
    "    url_X, url_y, test_size=0.2, random_state=42, stratify=url_y\n",
    ")\n",
    "print(f\"Train size: {len(url_X_train)}, Test size: {len(url_X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "url_results = {}\n",
    "print(\"Training and evaluating URL models...\")\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        pipeline.fit(url_X_train, url_y_train)\n",
    "        y_pred = pipeline.predict(url_X_test)\n",
    "        acc = accuracy_score(url_y_test, y_pred)\n",
    "        f1 = f1_score(url_y_test, y_pred)\n",
    "        url_results[name] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_score\": f1,\n",
    "            \"predictions\": y_pred,\n",
    "            \"model\": pipeline,\n",
    "        }\n",
    "        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compare URL model performances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "comparison_url = []\n",
    "for name, result in url_results.items():\n",
    "    comparison_url.append(\n",
    "        {\"Model\": name, \"Accuracy\": result[\"accuracy\"], \"F1 Score\": result[\"f1_score\"]}\n",
    "    )\n",
    "\n",
    "comparison_url_df = pd.DataFrame(comparison_url).sort_values(\n",
    "    \"Accuracy\", ascending=False\n",
    ")\n",
    "print(comparison_url_df.to_string(index=False))\n",
    "\n",
    "best_url_model_name = comparison_url_df.iloc[0][\"Model\"]\n",
    "best_url_model = url_results[best_url_model_name][\"model\"]\n",
    "print(f\"\\nBest URL model: {best_url_model_name}\")\n",
    "print(f\"Accuracy: {comparison_url_df.iloc[0]['Accuracy']:.4f}\")\n",
    "\n",
    "# Visualize accuracy and F1 score\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(\n",
    "    comparison_url_df[\"Model\"],\n",
    "    comparison_url_df[\"Accuracy\"],\n",
    "    color=\"skyblue\",\n",
    "    label=\"Accuracy\",\n",
    ")\n",
    "plt.bar(\n",
    "    comparison_url_df[\"Model\"],\n",
    "    comparison_url_df[\"F1 Score\"],\n",
    "    color=\"lightcoral\",\n",
    "    alpha=0.7,\n",
    "    label=\"F1 Score\",\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Phishing URL Model Performance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for best model\n",
    "y_pred_best = url_results[best_url_model_name][\"predictions\"]\n",
    "cm = confusion_matrix(url_y_test, y_pred_best)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Legitimate\", \"Phishing\"],\n",
    "    yticklabels=[\"Legitimate\", \"Phishing\"],\n",
    ")\n",
    "plt.title(f\"Confusion Matrix - {best_url_model_name}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a function to predict URL status\n",
    "def predict_url_status(url, model=None):\n",
    "    \"\"\"\n",
    "    Predict if a URL is phishing (1) or legitimate (0) based on its text.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to predict\n",
    "    model: The trained model to use for prediction (default: best model)\n",
    "\n",
    "    Returns:\n",
    "    dict: Prediction result with probability scores\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = best_url_model\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict([url])[0]\n",
    "\n",
    "    # Get prediction probabilities\n",
    "    probabilities = model.predict_proba([url])[0]\n",
    "\n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        \"prediction\": prediction,\n",
    "        \"status\": \"Legitimate\" if prediction == 1 else \"Phishing\",\n",
    "        \"confidence\": max(probabilities),\n",
    "        \"probability_legitimate\": probabilities[0],\n",
    "        \"probability_phishing\": probabilities[1],\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "url_sample = [\n",
    "    \"google.com\",\n",
    "    \"facebook.com\",\n",
    "    \"phishing-test.com\",\n",
    "    \"example.com\",\n",
    "    \"malicious-site.com\",\n",
    "    \"facebook-test.com\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting URL prediction function:\")\n",
    "for url in url_sample:\n",
    "    result = predict_url_status(url)\n",
    "    print(\n",
    "        f\"URL: {url} | Prediction: {result['status']} | \"\n",
    "        f\"Confidence: {result['confidence']:.4f} | \"\n",
    "        f\"Prob Legitimate: {result['probability_legitimate']:.4f} | \"\n",
    "        f\"Prob Phishing: {result['probability_phishing']:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8021783,
     "sourceId": 12693261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8055177,
     "sourceId": 12742804,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8060741,
     "sourceId": 12751295,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
